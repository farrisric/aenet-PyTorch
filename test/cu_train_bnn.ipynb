{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/farri002/miniconda3/envs/tyxe/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import resource\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_classes import *\n",
    "from read_input import *\n",
    "from read_trainset import *\n",
    "from network import *\n",
    "from prepare_batches import *\n",
    "from traininit import *\n",
    "from data_set import *\n",
    "from data_loader import *\n",
    "from optimization_step import *\n",
    "from output_nn import *\n",
    "from py_aeio import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "tin_file = \"train.in\"\n",
    "tin = read_train_in(tin_file)\n",
    "torch.manual_seed(3)\n",
    "np.random.seed(tin.numpy_seed)\n",
    "tin.train_forces = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tin.train_file = 'Cu.active_learning'\n",
    "list_structures_energy, _, list_removed, max_nnb, tin = read_list_structures(tin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnn import BayesianNeuralNetwork\n",
    "from bnn import get_batch\n",
    "\n",
    "net = NetAtom(tin.networks_param[\"input_size\"], tin.networks_param[\"hidden_size\"],\n",
    "\t\t\t    tin.sys_species, tin.networks_param[\"activations\"], tin.alpha, device)\n",
    "\n",
    "bnn = BayesianNeuralNetwork(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "dataset_size = len(list_structures_energy)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices = indices[:5000]\n",
    "test_indices = indices[5000:6000]\n",
    "valid_indices = indices[6000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_structures_energy = [list_structures_energy[x] for x in training_indices]\n",
    "test_structures_energy     = [list_structures_energy[x] for x in test_indices]\n",
    "valid_structure_energy     = [list_structures_energy[x] for x in valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = get_batch(tin, training_structures_energy, max_nnb)\n",
    "test_batch     = get_batch(tin, test_structures_energy, max_nnb)\n",
    "valid_batch    = get_batch(tin, valid_structure_energy, max_nnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH LOSS 0001] loss: 10.5788\n",
      "[EPOCH RMSD 0001] loss: 183.7037\n",
      "[EPOCH LOSS 0101] loss: 1.8492\n",
      "[EPOCH LOSS 0201] loss: 4.8630\n",
      "[EPOCH LOSS 0301] loss: 2.0905\n",
      "[EPOCH LOSS 0401] loss: 1.5916\n",
      "[EPOCH LOSS 0501] loss: 4.4344\n",
      "[EPOCH LOSS 0601] loss: 1.5649\n",
      "[EPOCH LOSS 0701] loss: 1.6004\n",
      "[EPOCH LOSS 0801] loss: 1.5291\n",
      "[EPOCH LOSS 0901] loss: 15.7572\n",
      "[EPOCH LOSS 1001] loss: 2.0339\n",
      "[EPOCH RMSD 1001] loss: 296.1186\n",
      "[EPOCH LOSS 1101] loss: 1.6032\n",
      "[EPOCH LOSS 1201] loss: 2.0730\n",
      "[EPOCH LOSS 1301] loss: 2.0113\n",
      "[EPOCH LOSS 1401] loss: 1.7114\n",
      "[EPOCH LOSS 1501] loss: 1.6081\n",
      "[EPOCH LOSS 1601] loss: 1.6360\n",
      "[EPOCH LOSS 1701] loss: 3.0816\n",
      "[EPOCH LOSS 1801] loss: 5.3890\n",
      "[EPOCH LOSS 1901] loss: 1.7784\n",
      "[EPOCH LOSS 2001] loss: 1.3785\n",
      "[EPOCH RMSD 2001] loss: 112.7052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bnn\u001b[39m.\u001b[39;49mtrain(training_batch, EPOCHS, initial_lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/bin/forks/aenet-PyTorch/test/../src/bnn.py:86\u001b[0m, in \u001b[0;36mBayesianNeuralNetwork.train\u001b[0;34m(self, grouped_train_loader, epochs, initial_lr, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(logic_reduce)):\n\u001b[1;32m     84\u001b[0m         logic_reduce[i] \u001b[39m=\u001b[39m logic_reduce[i]\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 86\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m svi\u001b[39m.\u001b[39;49mstep(grp_descrp, logic_reduce, grp_energy) \n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m j \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tyxe/lib/python3.9/site-packages/pyro/infer/svi.py:153\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39munconstrained() \u001b[39mfor\u001b[39;00m site \u001b[39min\u001b[39;00m param_capture\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mnodes\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[39m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim(params)\n\u001b[1;32m    155\u001b[0m \u001b[39m# zero gradients\u001b[39;00m\n\u001b[1;32m    156\u001b[0m pyro\u001b[39m.\u001b[39minfer\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mzero_grads(params)\n",
      "File \u001b[0;32m~/miniconda3/envs/tyxe/lib/python3.9/site-packages/pyro/optim/optim.py:142\u001b[0m, in \u001b[0;36mPyroOptim.__call__\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_clip[p] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_clip[p](p)\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_objs[p], torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mlr_scheduler\u001b[39m.\u001b[39m_LRScheduler\n\u001b[1;32m    143\u001b[0m ) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_objs[p], torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau\n\u001b[1;32m    145\u001b[0m ):\n\u001b[1;32m    146\u001b[0m     \u001b[39m# if optim object was a scheduler, perform an optimizer step\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_objs[p]\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bnn.train(training_batch, EPOCHS, initial_lr=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = bnn.predict(valid_batch,num_samples=1000)\n",
    "std_valid_batch = torch.std(valid_pred['obs'],0)\n",
    "\n",
    "test_pred = bnn.predict(test_batch,num_samples=1000)\n",
    "std_test_batch = torch.std(test_pred['obs'],0)\n",
    "idx_test_sorted = np.argsort(std_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = test_indices[idx_test_sorted[-1]]\n",
    "new_structure_energy = [list_structures_energy[idx]]\n",
    "new_training_structures_energy = training_structures_energy + new_structure_energy\n",
    "new_training_batch = get_batch(tin, new_training_structures_energy, max_nnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSD valid set pre train (tensor(93.3421, dtype=torch.float64), tensor(365.9643, dtype=torch.float64))\n",
      "(tensor(44.0775, dtype=torch.float64), tensor(351.6858, dtype=torch.float64)) tensor(2.3626, dtype=torch.float64) tensor(1.9920, dtype=torch.float64)\n",
      "(tensor(32.2377, dtype=torch.float64), tensor(228.9130, dtype=torch.float64)) tensor(2.5639, dtype=torch.float64) tensor(1.8053, dtype=torch.float64)\n",
      "(tensor(21.5564, dtype=torch.float64), tensor(185.1047, dtype=torch.float64)) tensor(2.6573, dtype=torch.float64) tensor(1.5650, dtype=torch.float64)\n",
      "(tensor(30.2174, dtype=torch.float64), tensor(235.5293, dtype=torch.float64)) tensor(2.7478, dtype=torch.float64) tensor(1.6847, dtype=torch.float64)\n",
      "(tensor(32.5660, dtype=torch.float64), tensor(232.7162, dtype=torch.float64)) tensor(2.7985, dtype=torch.float64) tensor(1.8011, dtype=torch.float64)\n",
      "(tensor(39.1014, dtype=torch.float64), tensor(227.1222, dtype=torch.float64)) tensor(2.8514, dtype=torch.float64) tensor(1.9808, dtype=torch.float64)\n",
      "(tensor(27.4380, dtype=torch.float64), tensor(220.6372, dtype=torch.float64)) tensor(2.9147, dtype=torch.float64) tensor(1.7925, dtype=torch.float64)\n",
      "(tensor(28.3084, dtype=torch.float64), tensor(227.5491, dtype=torch.float64)) tensor(3.0967, dtype=torch.float64) tensor(1.8731, dtype=torch.float64)\n",
      "(tensor(40.2195, dtype=torch.float64), tensor(281.7873, dtype=torch.float64)) tensor(3.2097, dtype=torch.float64) tensor(1.9760, dtype=torch.float64)\n",
      "(tensor(21.2508, dtype=torch.float64), tensor(156.7773, dtype=torch.float64)) tensor(3.2935, dtype=torch.float64) tensor(1.5703, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "l2 = bnn.get_loss_RMSE(valid_batch, num_samples=NUM_SAMPLES)\n",
    "print('RMSD valid set pre train {}'.format(l2))\n",
    "\n",
    "with open('std_test.txt', 'w') as out:\n",
    "    for x in range(0,1000,100): \n",
    "        \n",
    "        idx = test_indices[idx_test_sorted[x]]\n",
    "        std_new_point = std_test_batch[idx_test_sorted[x]]\n",
    "        new_structure_energy = [list_structures_energy[idx]]\n",
    "        new_training_structures_energy = training_structures_energy + new_structure_energy\n",
    "        new_training_batch = get_batch(tin, new_training_structures_energy, max_nnb)\n",
    "\n",
    "        bnn1 = copy.deepcopy(bnn)\n",
    "        bnn1.train(new_training_batch, int(EPOCHS/ ), initial_lr=0.01, verbose=False)\n",
    "\n",
    "        valid_pred = bnn1.predict(valid_batch,num_samples=NUM_SAMPLES)\n",
    "        std_valid_batch = torch.mean(torch.std(valid_pred['obs'],0))\n",
    "        l2 = bnn1.get_loss_RMSE(valid_batch, num_samples=NUM_SAMPLES)\n",
    "        out.write('{} {} {}\\n'.format(std_new_point, std_valid_batch, l2))\n",
    "        print(l2, std_new_point, std_valid_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x.txt', 'w') as out:\n",
    "    for x in range(10):\n",
    "        out.write('{}\\n'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tyxe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
