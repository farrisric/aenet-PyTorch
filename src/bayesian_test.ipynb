{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import resource\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_classes import *\n",
    "from read_input import *\n",
    "from read_trainset import *\n",
    "from network import *\n",
    "from prepare_batches import *\n",
    "from traininit import *\n",
    "from data_set import *\n",
    "from data_loader import *\n",
    "from optimization_step import *\n",
    "from output_nn import *\n",
    "from py_aeio import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tin_file = \"/nethome/farri002/mlp-pt-au-o-h/ml-construction/PdO/desc-20-8/NN-20-20/train.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "                       Reading input information                      \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Reading input parameters.\n",
      "These are the parameters selected for training:\n",
      "        - TRAININGSET: TiO.train.ascii\n",
      "        - TESTPERCENT: 0.1\n",
      "        - ITERATIONS:  500\n",
      "        - ITERWRITE:   1\n",
      "        - BATCHSIZE:   128\n",
      "        - MEMORY_MODE: cpu\n",
      "\n",
      "        - FORCES:      True\n",
      "        - alpha:       0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tin_file = \"/nethome/farri002/bin/forks/aenet-PyTorch/example/02-train/train.in\"\n",
    "tin = read_train_in(tin_file)\n",
    "torch.manual_seed(3)\n",
    "np.random.seed(tin.numpy_seed)\n",
    "if tin.verbose: io_input_reading(tin)\n",
    "tin.train_file = '/nethome/farri002/bin/forks/aenet-PyTorch/example/01-generate/TiO.train.ascii'\n",
    "tin.train_forces_file = '/nethome/farri002/bin/forks/aenet-PyTorch/example/01-generate/TiO.train.forces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_structures_energy, list_structures_forces, list_removed, max_nnb, tin = read_list_structures(tin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_removed = len(list_removed)\n",
    "N_struc_E = len(list_structures_energy)\n",
    "N_struc_F = len(list_structures_forces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_batches(tin, trainset_params, device, list_structures_energy, list_structures_forces,\n",
    "\t\t\t\t   max_nnb, N_batch_train, N_batch_test, N_batch_valid, train_sampler_E, test_sampler_E, valid_sampler_E):\n",
    "        \"\"\"\n",
    "        Select which structures belong to each batch for training.\n",
    "        Returns: four objects of the class data_set_loader.PrepDataloader(), for train/test and energy/forces\n",
    "        \"\"\"\n",
    "        dataset_energy = StructureDataset(list_structures_energy, tin.sys_species, tin.networks_param[\"input_size\"], max_nnb)\n",
    "        \n",
    "        dataset_energy_size = len(dataset_energy)\n",
    "\n",
    "        # Normalize\n",
    "        E_scaling, E_shift = tin.trainset_params.E_scaling, tin.trainset_params.E_shift\n",
    "        sfval_avg, sfval_cov = tin.setup_params.sfval_avg, tin.setup_params.sfval_cov\n",
    "        dataset_energy.normalize_E(trainset_params.E_scaling, trainset_params.E_shift)\n",
    "        stp_shift, stp_scale = dataset_energy.normalize_stp(sfval_avg, sfval_cov)\n",
    "\n",
    "        # Split in train/test\n",
    "        #train_sampler_E, valid_sampler_E = split_database(dataset_energy_size, tin.test_split)\n",
    "\n",
    "        train_energy_data = PrepDataloader(dataset=dataset_energy, train_forces=False, N_batch=N_batch_train,\n",
    "                                        sampler=train_sampler_E, memory_mode=tin.memory_mode, device=device, dataname=\"train_energy\")\n",
    "        test_energy_data = PrepDataloader(dataset=dataset_energy, train_forces=False, N_batch=N_batch_train,\n",
    "                                        sampler=test_sampler_E, memory_mode=tin.memory_mode, device=device, dataname=\"test_energy\")\n",
    "\n",
    "        if valid_sampler_E:\n",
    "            valid_energy_data = PrepDataloader(dataset=dataset_energy, train_forces=False, N_batch=N_batch_valid,\n",
    "                                        sampler=valid_sampler_E, memory_mode=tin.memory_mode, device=device, dataname=\"valid_energy\")\n",
    "            return train_energy_data, test_energy_data, valid_energy_data\n",
    "        \n",
    "        return train_energy_data, test_energy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets_from_indices(training_indices, test_indices, valid_indices, list_structures_energy):\n",
    "    N_batch_train, N_batch_test, N_batch_valid = 1, 1, 1 #select_batch_size(tin, list_structures_energy, list_structures_forces)\n",
    "\n",
    "    # Join datasets with forces and only energies in a single torch dataset AND prepare batches\n",
    "    if valid_indices:\n",
    "        train_energy_data, test_energy_data, valid_energy_data = select_batches(tin, tin.trainset_params, device, list_structures_energy, None,\n",
    "                    max_nnb, N_batch_train,N_batch_test, N_batch_valid, training_indices, test_indices, valid_indices)\n",
    "    else:\n",
    "        train_energy_data, test_energy_data = select_batches(tin, tin.trainset_params, device, list_structures_energy, None,\n",
    "                    max_nnb, N_batch_train,N_batch_test, N_batch_valid, training_indices, test_indices, valid_indices)\n",
    "        \n",
    "    grouped_train_data = GroupedDataset(train_energy_data, None,\n",
    "\t\t\t\t\t\t\t\t\t memory_mode=tin.memory_mode, device=device, dataname=\"train\")\n",
    "    grouped_test_data = GroupedDataset(test_energy_data, None,\n",
    "\t\t\t\t\t\t\t\t\t memory_mode=tin.memory_mode, device=device, dataname=\"test\")\n",
    "    \n",
    "    grouped_train_loader = DataLoader(grouped_train_data, batch_size=1, shuffle=False,\n",
    "                                  collate_fn=custom_collate, num_workers=0)\n",
    "    grouped_test_loader = DataLoader(grouped_test_data, batch_size=1, shuffle=False,\n",
    "                                  collate_fn=custom_collate, num_workers=0)\n",
    "\n",
    "    if valid_indices:\n",
    "        grouped_valid_data = GroupedDataset(valid_energy_data, None,\n",
    "\t\t\t\t\t\t\t\t\t    memory_mode=tin.memory_mode, device=device, dataname=\"valid\")\n",
    "        grouped_valid_loader = DataLoader(grouped_valid_data, batch_size=1, shuffle=False,\n",
    "                                    collate_fn=custom_collate, num_workers=0)\n",
    "        \n",
    "        return grouped_train_loader, grouped_test_loader, grouped_valid_loader\n",
    "    \n",
    "    return grouped_train_loader, grouped_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "dataset_size = len(list_structures_energy)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices, test_indices, valid_indices = indices[:650], indices[650:670], indices[670:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "grouped_train_loader, grouped_test_loader, grouped_valid_loader = get_sets_from_indices(training_indices, test_indices, valid_indices, copy.deepcopy(list_structures_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(grouped_train_loader):\n",
    "    for data_batch in grouped_train_loader:\n",
    "        grp_descrp, grp_energy, logic_reduce, grp_N_atoms = data_batch[0][10], data_batch[0][11], data_batch[0][12],data_batch[0][14]\n",
    "\n",
    "    grp_descrp[0] = grp_descrp[0].float()\n",
    "    grp_descrp[1] = grp_descrp[1].float()\n",
    "\n",
    "    logic_reduce[0] = logic_reduce[0].float()\n",
    "    logic_reduce[1] = logic_reduce[1].float()\n",
    "\n",
    "    return grp_descrp, grp_energy, logic_reduce, grp_N_atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v  in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0).numpy(),\n",
    "            \"std\": torch.std(v, 0).numpy(),\n",
    "            \"5%\": v.kthvalue(int(len(v) * 0.05), dim=0)[0].numpy(),\n",
    "            \"95%\": v.kthvalue(int(len(v) * 0.95), dim=0)[0].numpy(),\n",
    "        }\n",
    "    return site_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnn import BayesianNeuralNetwork\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import Predictive\n",
    "import pyro\n",
    "\n",
    "net = NetAtom(tin.networks_param[\"input_size\"], tin.networks_param[\"hidden_size\"],\n",
    "\t\t\t    tin.sys_species, tin.networks_param[\"activations\"], tin.alpha, device)\n",
    "\n",
    "bnn = BayesianNeuralNetwork(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_grp_descrp, v_grp_energy, v_logic_reduce, v_grp_N_atoms = get_train_test(grouped_valid_loader) \n",
    "t_grp_descrp, t_grp_energy, t_logic_reduce, t_grp_N_atoms = get_train_test(grouped_test_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epochs: 150000\n",
      "loss: 6.442596814291762\n",
      "Std Valid Set: 2.141653437818345\n",
      "RMSD: 2.5522304161989795 +- 1.2398254975827705\n",
      " \n",
      "Epochs: 160000\n",
      "loss: 2.934505159575083\n",
      "Std Valid Set: 2.1240868126145895\n",
      "RMSD: 2.53533429111407 +- 1.5793293239261244\n",
      " \n",
      "Epochs: 170000\n",
      "loss: 2.526634696552715\n",
      "Std Valid Set: 2.056527231734015\n",
      "RMSD: 2.4754785026919484 +- 1.5783095101654219\n",
      " \n",
      "Epochs: 180000\n",
      "loss: 2.4731265591537146\n",
      "Std Valid Set: 2.355816084897385\n",
      "RMSD: 2.4531397124076184 +- 1.7074034547341037\n",
      " \n",
      "Epochs: 190000\n",
      "loss: 2.7274045096437307\n",
      "Std Valid Set: 2.0731383056385204\n",
      "RMSD: 2.4096378053338814 +- 1.363494939815068\n",
      " \n",
      "Epochs: 200000\n",
      "loss: 2.4899567276352315\n",
      "Std Valid Set: 2.14549511273274\n",
      "RMSD: 2.328349932175319 +- 1.4822601329544678\n",
      " \n",
      "Epochs: 210000\n",
      "loss: 2.497458184528762\n",
      "Std Valid Set: 1.9757761262808842\n",
      "RMSD: 2.2992955819414247 +- 1.8279626179560577\n",
      " \n",
      "Epochs: 220000\n",
      "loss: 3.4269197478007074\n",
      "Std Valid Set: 2.079203725289267\n",
      "RMSD: 2.237810065600217 +- 2.5988219853113432\n",
      " \n",
      "Epochs: 230000\n",
      "loss: 2.486922294828379\n",
      "Std Valid Set: 2.006845784360577\n",
      "RMSD: 2.1958549732288457 +- 2.1129517097459725\n",
      " \n",
      "Epochs: 240000\n",
      "loss: 2.565713737936016\n",
      "Std Valid Set: 1.8939185829510818\n",
      "RMSD: 2.2702822688316497 +- 2.898478685159293\n",
      " \n",
      "Epochs: 250000\n",
      "loss: 2.405082467535928\n",
      "Std Valid Set: 2.1801013458683163\n",
      "RMSD: 2.2705088069723818 +- 3.204382813373871\n",
      " \n",
      "Epochs: 260000\n",
      "loss: 2.3072546315201348\n",
      "Std Valid Set: 1.8278783060199288\n",
      "RMSD: 2.184931654736461 +- 3.743558770878835\n",
      " \n",
      "Epochs: 270000\n",
      "loss: 2.3790019833933402\n",
      "Std Valid Set: 1.953755381188482\n",
      "RMSD: 2.0568256109456216 +- 3.844613149253447\n",
      " \n",
      "Epochs: 280000\n",
      "loss: 2.2554948723688364\n",
      "Std Valid Set: 2.1345039306712765\n",
      "RMSD: 1.993433128873588 +- 3.239969571994854\n",
      " \n",
      "Epochs: 290000\n",
      "loss: 2.1570399647442797\n",
      "Std Valid Set: 1.9554213908903826\n",
      "RMSD: 1.824729924796659 +- 2.8410670417382495\n",
      " \n",
      "Epochs: 300000\n",
      "loss: 2.347703165120716\n",
      "Std Valid Set: 2.0536703545839456\n",
      "RMSD: 1.7520657240798057 +- 3.9764601277303124\n",
      " \n",
      "Epochs: 310000\n",
      "loss: 2.1917331683953267\n",
      "Std Valid Set: 1.8557644844191625\n",
      "RMSD: 1.5799627031764485 +- 3.0962393461005417\n",
      " \n",
      "Epochs: 320000\n",
      "loss: 2.373249937963995\n",
      "Std Valid Set: 1.8846200606504486\n",
      "RMSD: 1.515889163770057 +- 2.7596710839903107\n",
      " \n",
      "Epochs: 330000\n",
      "loss: 2.3011168605988868\n",
      "Std Valid Set: 1.4805756908992072\n",
      "RMSD: 1.471025979358446 +- 2.862006045223976\n",
      " \n",
      "Epochs: 340000\n",
      "loss: 2.09896657054155\n",
      "Std Valid Set: 1.9200877589117638\n",
      "RMSD: 1.4045468282234796 +- 2.3491841076386155\n",
      " \n",
      "Epochs: 350000\n",
      "loss: 2.233853742873548\n",
      "Std Valid Set: 1.5854250869773814\n",
      "RMSD: 1.3629940401340672 +- 2.420729285160742\n",
      " \n",
      "Epochs: 360000\n",
      "loss: 2.1664272535769964\n",
      "Std Valid Set: 1.8424891430723647\n",
      "RMSD: 1.395157723789304 +- 3.7003407491787277\n",
      " \n",
      "Epochs: 370000\n",
      "loss: 2.165497182407243\n",
      "Std Valid Set: 1.2436946519266328\n",
      "RMSD: 1.317634912785847 +- 3.3115196997632195\n",
      " \n",
      "Epochs: 380000\n",
      "loss: 2.341954817120409\n",
      "Std Valid Set: 1.3010882216790436\n",
      "RMSD: 1.331360357922917 +- 3.28696494576053\n",
      " \n",
      "Epochs: 390000\n",
      "loss: 2.3148412833921177\n",
      "Std Valid Set: 1.217813969961076\n",
      "RMSD: 1.3144087785721799 +- 1.754791812915769\n",
      " \n",
      "Epochs: 400000\n",
      "loss: 2.300590836881133\n",
      "Std Valid Set: 1.5354845736114302\n",
      "RMSD: 1.373085428864205 +- 3.6746948905919017\n",
      " \n",
      "Epochs: 410000\n",
      "loss: 2.554154105241961\n",
      "Std Valid Set: 1.9642114840214582\n",
      "RMSD: 1.3860583826824164 +- 3.566159239091058\n",
      " \n",
      "Epochs: 420000\n",
      "loss: 2.0507118977757215\n",
      "Std Valid Set: 1.272987749374624\n",
      "RMSD: 1.2672224046794442 +- 1.5926442599863595\n",
      " \n",
      "Epochs: 430000\n",
      "loss: 2.128339658290822\n",
      "Std Valid Set: 1.2822981810949714\n",
      "RMSD: 1.3138570141041752 +- 2.086261473816692\n",
      " \n",
      "Epochs: 440000\n",
      "loss: 2.281162686258979\n",
      "Std Valid Set: 1.6787687768757167\n",
      "RMSD: 1.3727317569927413 +- 3.6291222987038205\n",
      " \n",
      "Epochs: 450000\n",
      "loss: 2.1662244322541477\n",
      "Std Valid Set: 1.3904628023165089\n",
      "RMSD: 1.3736754881979962 +- 3.9107434403769474\n",
      " \n",
      "Epochs: 460000\n",
      "loss: 2.058229914485064\n",
      "Std Valid Set: 1.1328071338412877\n",
      "RMSD: 1.4191742746193472 +- 3.6134419929592227\n",
      " \n",
      "Epochs: 470000\n",
      "loss: 2.182430632910359\n",
      "Std Valid Set: 1.281182583289043\n",
      "RMSD: 1.3084339584063989 +- 2.3652525527890154\n",
      " \n",
      "Epochs: 480000\n",
      "loss: 2.2637848931506634\n",
      "Std Valid Set: 1.1287626064994967\n",
      "RMSD: 1.3146172018679516 +- 3.03714602161762\n",
      " \n",
      "Epochs: 490000\n",
      "loss: 2.141542088667659\n",
      "Std Valid Set: 1.134403234008501\n",
      "RMSD: 1.257084996182676 +- 1.9302146025474685\n",
      " \n",
      "Epochs: 500000\n",
      "loss: 2.191111209499482\n",
      "Std Valid Set: 1.1320384315506271\n",
      "RMSD: 1.280867333691087 +- 3.150775410039485\n",
      " \n",
      "Epochs: 510000\n",
      "loss: 2.0739118694978607\n",
      "Std Valid Set: 1.381294947834536\n",
      "RMSD: 1.3358052786451449 +- 4.09817589302341\n",
      " \n",
      "Epochs: 520000\n",
      "loss: 2.2220280451163226\n",
      "Std Valid Set: 1.644571083079042\n",
      "RMSD: 1.2842881305736396 +- 3.259833966942686\n",
      " \n",
      "Epochs: 530000\n",
      "loss: 2.5088505839582536\n",
      "Std Valid Set: 1.5097580403345638\n",
      "RMSD: 1.2221520137711253 +- 2.5084556148599204\n",
      " \n",
      "Epochs: 540000\n",
      "loss: 2.2360618546526516\n",
      "Std Valid Set: 1.102390328028779\n",
      "RMSD: 1.2428250772796383 +- 3.21793741478783\n",
      " \n",
      "Epochs: 550000\n",
      "loss: 2.0810342953201575\n",
      "Std Valid Set: 1.1155215815512771\n",
      "RMSD: 1.213605479548727 +- 3.2150918171407183\n",
      " \n",
      "Epochs: 560000\n",
      "loss: 2.119922379913732\n",
      "Std Valid Set: 1.365207007266387\n",
      "RMSD: 1.0905095395925435 +- 2.926021014905828\n",
      " \n",
      "Epochs: 570000\n",
      "loss: 2.1518300355908857\n",
      "Std Valid Set: 1.3723914912996324\n",
      "RMSD: 1.129402137706521 +- 3.4693964339599477\n",
      " \n",
      "Epochs: 580000\n",
      "loss: 2.201100054649229\n",
      "Std Valid Set: 1.5476346583391833\n",
      "RMSD: 1.0941305681742999 +- 1.9157841251478749\n",
      " \n",
      "Epochs: 590000\n",
      "loss: 2.05995733137786\n",
      "Std Valid Set: 1.0465226093119369\n",
      "RMSD: 1.0764475886952138 +- 1.3255694702702718\n",
      " \n",
      "Epochs: 600000\n",
      "loss: 2.072386048235755\n",
      "Std Valid Set: 1.2212595122764562\n",
      "RMSD: 1.1438563823745274 +- 3.247351400875239\n",
      " \n",
      "Epochs: 610000\n",
      "loss: 2.1459991370245466\n",
      "Std Valid Set: 0.9549173169926608\n",
      "RMSD: 1.0763091581585584 +- 2.655305682657508\n",
      " \n",
      "Epochs: 620000\n",
      "loss: 1.9102167989553507\n",
      "Std Valid Set: 1.2677536687518987\n",
      "RMSD: 1.056484774051252 +- 2.3310125307182292\n",
      " \n",
      "Epochs: 630000\n",
      "loss: 1.968290560130698\n",
      "Std Valid Set: 1.6041654550272373\n",
      "RMSD: 1.0206168872259198 +- 3.0805615566566136\n",
      " \n",
      "Epochs: 640000\n",
      "loss: 2.0055724144766893\n",
      "Std Valid Set: 1.3235572492151166\n",
      "RMSD: 1.015479287940961 +- 2.702073628428276\n",
      " \n",
      "Epochs: 650000\n",
      "loss: 2.1107590567623564\n",
      "Std Valid Set: 1.2119711747854631\n",
      "RMSD: 1.0082744730277717 +- 2.943415385956572\n",
      " \n",
      "Epochs: 660000\n",
      "loss: 2.207097225134316\n",
      "Std Valid Set: 1.041904376526516\n",
      "RMSD: 0.981264303717588 +- 2.9916115260119414\n",
      " \n",
      "Epochs: 670000\n",
      "loss: 1.9879735669926966\n",
      "Std Valid Set: 0.9867996473683684\n",
      "RMSD: 0.8931204146997926 +- 1.6989127381012943\n",
      " \n",
      "Epochs: 680000\n",
      "loss: 2.084988672042217\n",
      "Std Valid Set: 0.9582497501507075\n",
      "RMSD: 0.9291764841211964 +- 3.240373721668602\n",
      " \n",
      "Epochs: 690000\n",
      "loss: 2.1531170328844342\n",
      "Std Valid Set: 1.3428018781440296\n",
      "RMSD: 0.8110386044114265 +- 1.971709366596513\n",
      " \n",
      "Epochs: 700000\n",
      "loss: 1.9412796364212983\n",
      "Std Valid Set: 0.82999562111146\n",
      "RMSD: 0.8235208132432823 +- 2.257059896426852\n",
      " \n",
      "Epochs: 710000\n",
      "loss: 2.342708557996945\n",
      "Std Valid Set: 0.7824275346202917\n",
      "RMSD: 0.8237108675102514 +- 2.370750838194119\n",
      " \n",
      "Epochs: 720000\n",
      "loss: 1.9098903221498529\n",
      "Std Valid Set: 0.8592262945232765\n",
      "RMSD: 0.8078782144894389 +- 2.6587309098533822\n",
      " \n",
      "Epochs: 730000\n",
      "loss: 2.0822776568737784\n",
      "Std Valid Set: 1.5870445131810977\n",
      "RMSD: 0.7986883200005072 +- 3.2262721982375946\n",
      " \n",
      "Epochs: 740000\n",
      "loss: 1.9583268050288167\n",
      "Std Valid Set: 1.3666154162923998\n",
      "RMSD: 0.7293687799141717 +- 2.3455678440630456\n",
      " \n",
      "Epochs: 750000\n",
      "loss: 1.83630536936617\n",
      "Std Valid Set: 1.2650476084416398\n",
      "RMSD: 0.7690421125532876 +- 3.3335197753747976\n",
      " \n",
      "Epochs: 760000\n",
      "loss: 1.9173092687996856\n",
      "Std Valid Set: 0.7579933057662785\n",
      "RMSD: 0.7524091258318152 +- 2.954315282052874\n",
      " \n",
      "Epochs: 770000\n",
      "loss: 1.907182195806784\n",
      "Std Valid Set: 0.8979047444354904\n",
      "RMSD: 0.6150089322982524 +- 1.636272261354444\n",
      " \n",
      "Epochs: 780000\n",
      "loss: 1.8713751330473232\n",
      "Std Valid Set: 1.0259896692459705\n",
      "RMSD: 0.5905235728469126 +- 1.478265399766315\n",
      " \n",
      "Epochs: 790000\n",
      "loss: 2.0695388738538365\n",
      "Std Valid Set: 0.8275327183674971\n",
      "RMSD: 0.6091282011739613 +- 2.0419274119351254\n",
      " \n",
      "Epochs: 800000\n",
      "loss: 1.9163353193087587\n",
      "Std Valid Set: 0.8238001619809527\n",
      "RMSD: 0.5830832006551089 +- 1.8637074462248624\n",
      " \n",
      "Epochs: 810000\n",
      "loss: 2.112662780990539\n",
      "Std Valid Set: 0.9421058225551343\n",
      "RMSD: 0.5811776647511641 +- 1.9036949043366944\n",
      " \n",
      "Epochs: 820000\n",
      "loss: 2.4956036556393864\n",
      "Std Valid Set: 1.1332184137984749\n",
      "RMSD: 0.5648059748611471 +- 2.3932022495606526\n",
      " \n",
      "Epochs: 830000\n",
      "loss: 2.35978597104755\n",
      "Std Valid Set: 0.7475077042197136\n",
      "RMSD: 0.5134619649842247 +- 2.1131841495777075\n",
      " \n",
      "Epochs: 840000\n",
      "loss: 1.9259262335129512\n",
      "Std Valid Set: 1.2566796100324074\n",
      "RMSD: 0.5087654030979546 +- 1.586733341775675\n",
      " \n",
      "Epochs: 850000\n",
      "loss: 1.9786261899341053\n",
      "Std Valid Set: 0.6255554051243487\n",
      "RMSD: 0.506210061833721 +- 1.678865259981141\n",
      " \n",
      "Epochs: 860000\n",
      "loss: 2.014905656939197\n",
      "Std Valid Set: 0.6652675728696147\n",
      "RMSD: 0.4900857920016505 +- 2.2079659303738843\n",
      " \n",
      "Epochs: 870000\n",
      "loss: 1.844420526755653\n",
      "Std Valid Set: 0.8218515920816923\n",
      "RMSD: 0.48291187950286346 +- 1.6578086051449075\n",
      " \n",
      "Epochs: 880000\n",
      "loss: 2.1852629629256235\n",
      "Std Valid Set: 0.9052278260316584\n",
      "RMSD: 0.45416026266224574 +- 1.3263907868327414\n",
      " \n",
      "Epochs: 890000\n",
      "loss: 1.7261467493751312\n",
      "Std Valid Set: 0.8028645204199656\n",
      "RMSD: 0.45742660992252876 +- 1.7260388821260044\n",
      " \n",
      "Epochs: 900000\n",
      "loss: 1.7615614245920412\n",
      "Std Valid Set: 0.7658735832842735\n",
      "RMSD: 0.4417432411770886 +- 1.7194644994788633\n",
      " \n",
      "Epochs: 910000\n",
      "loss: 1.967839572833735\n",
      "Std Valid Set: 1.109050063495714\n",
      "RMSD: 0.48066466095820476 +- 2.3563788308893905\n",
      " \n",
      "Epochs: 920000\n",
      "loss: 1.7022244015832404\n",
      "Std Valid Set: 0.9968014144640209\n",
      "RMSD: 0.46011730052400224 +- 2.2145962099266026\n",
      " \n",
      "Epochs: 930000\n",
      "loss: 1.7270120133959266\n",
      "Std Valid Set: 0.5989313186228926\n",
      "RMSD: 0.42485601474891893 +- 1.903211737402811\n",
      " \n",
      "Epochs: 940000\n",
      "loss: 1.8057852065717963\n",
      "Std Valid Set: 0.7951929245041651\n",
      "RMSD: 0.4458028455860588 +- 2.146680791016715\n",
      " \n",
      "Epochs: 950000\n",
      "loss: 1.6908090137323177\n",
      "Std Valid Set: 1.0066846617422125\n",
      "RMSD: 0.4042087488164008 +- 1.3595552322006794\n",
      " \n",
      "Epochs: 960000\n",
      "loss: 2.096007891074285\n",
      "Std Valid Set: 0.9914900924883993\n",
      "RMSD: 0.38449923137818526 +- 1.7190609717163159\n",
      " \n",
      "Epochs: 970000\n",
      "loss: 1.8198995089473609\n",
      "Std Valid Set: 1.1962308673307986\n",
      "RMSD: 0.3816904400420865 +- 2.134043894096266\n",
      " \n",
      "Epochs: 980000\n",
      "loss: 1.649001106870195\n",
      "Std Valid Set: 0.6795268821363647\n",
      "RMSD: 0.3732033092980744 +- 1.4288151092747834\n",
      " \n",
      "Epochs: 990000\n",
      "loss: 1.8771613945345669\n",
      "Std Valid Set: 0.9376357674516307\n",
      "RMSD: 0.3860237009042027 +- 1.8461231957874742\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,100):\n",
    "    loss = bnn.train(grouped_train_loader, 10000)\n",
    "\n",
    "    predictions = bnn.predict(v_grp_descrp, v_logic_reduce, num_samples=800)\n",
    "    std = torch.std(predictions['obs'], 0).numpy()\n",
    "    l2, std_l2 = bnn.get_loss_RMSE(v_grp_descrp, v_grp_energy, v_logic_reduce, v_grp_N_atoms, 10000)\n",
    "\n",
    "    print(' ')\n",
    "    print('Epochs: {}'.format(i*10000))\n",
    "    print('loss: {}'.format(loss))\n",
    "    print('Std Valid Set: {}'.format(np.mean(std)))\n",
    "    print('RMSD: {} +- {}'.format(l2, std_l2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Samples: 1000\n",
      "Mean value obs: -13.131354121911505\n",
      "STD obs: 0.654095921256454\n",
      "Mean value obs: -13.110956069946289\n",
      "STD obs: 0.4167194912896756\n",
      "\n",
      "Number of Samples: 2000\n",
      "Mean value obs: -13.124368621049998\n",
      "STD obs: 0.671856899313604\n",
      "Mean value obs: -13.116442487716675\n",
      "STD obs: 0.4257772129384873\n",
      "\n",
      "Number of Samples: 3000\n",
      "Mean value obs: -13.118869058179847\n",
      "STD obs: 0.8574290701068339\n",
      "Mean value obs: -13.116594337463379\n",
      "STD obs: 0.6817847447063423\n",
      "\n",
      "Number of Samples: 4000\n",
      "Mean value obs: -13.125151220920529\n",
      "STD obs: 0.7192836934499853\n",
      "Mean value obs: -13.114072032749652\n",
      "STD obs: 0.5159731383263086\n",
      "\n",
      "Number of Samples: 5000\n",
      "Mean value obs: -13.100050400678601\n",
      "STD obs: 0.708620411133381\n",
      "Mean value obs: -13.108314805603028\n",
      "STD obs: 0.48953026429733726\n",
      "\n",
      "Number of Samples: 6000\n",
      "Mean value obs: -13.110401888990156\n",
      "STD obs: 0.7876330574037411\n",
      "Mean value obs: -13.11762721979618\n",
      "STD obs: 0.6085350247619573\n",
      "\n",
      "Number of Samples: 7000\n",
      "Mean value obs: -13.122082464748484\n",
      "STD obs: 0.8396672206943545\n",
      "Mean value obs: -13.119004948616027\n",
      "STD obs: 0.6603340129764862\n",
      "\n",
      "Number of Samples: 8000\n",
      "Mean value obs: -13.12879548465911\n",
      "STD obs: 0.8112024130281309\n",
      "Mean value obs: -13.11964918372035\n",
      "STD obs: 0.6288017700430407\n",
      "\n",
      "Number of Samples: 9000\n",
      "Mean value obs: -13.110896973112393\n",
      "STD obs: 0.7232775947844241\n",
      "Mean value obs: -13.113633553557925\n",
      "STD obs: 0.5115341010886959\n",
      "\n",
      "Number of Samples: 10000\n",
      "Mean value obs: -13.124518399561273\n",
      "STD obs: 0.811334014682605\n",
      "Mean value obs: -13.124857248282433\n",
      "STD obs: 0.6293815645507318\n",
      "\n",
      "Number of Samples: 11000\n",
      "Mean value obs: -13.113915226686261\n",
      "STD obs: 0.7888113021786846\n",
      "Mean value obs: -13.11740688846328\n",
      "STD obs: 0.6096266131002608\n",
      "\n",
      "Number of Samples: 12000\n",
      "Mean value obs: -13.120282054050953\n",
      "STD obs: 0.8296810218626566\n",
      "Mean value obs: -13.122135801533858\n",
      "STD obs: 0.6575733323558348\n",
      "\n",
      "Number of Samples: 13000\n",
      "Mean value obs: -13.116463482235057\n",
      "STD obs: 0.7561414404353479\n",
      "Mean value obs: -13.115159896740547\n",
      "STD obs: 0.5692689203465277\n",
      "\n",
      "Number of Samples: 14000\n",
      "Mean value obs: -13.111917745981494\n",
      "STD obs: 0.6719544671805728\n",
      "Mean value obs: -13.110250918166978\n",
      "STD obs: 0.44614890721920347\n",
      "\n",
      "Number of Samples: 15000\n",
      "Mean value obs: -13.110154684333414\n",
      "STD obs: 0.7424217448440453\n",
      "Mean value obs: -13.11338935848872\n",
      "STD obs: 0.5428871127982485\n",
      "\n",
      "Number of Samples: 16000\n",
      "Mean value obs: -13.119362576622306\n",
      "STD obs: 0.8121520468748745\n",
      "Mean value obs: -13.120722935020924\n",
      "STD obs: 0.6322853623320038\n",
      "\n",
      "Number of Samples: 17000\n",
      "Mean value obs: -13.112802715728762\n",
      "STD obs: 0.7573740045769543\n",
      "Mean value obs: -13.115937251876382\n",
      "STD obs: 0.570040420370003\n",
      "\n",
      "Number of Samples: 18000\n",
      "Mean value obs: -13.112468621346117\n",
      "STD obs: 0.7218365467833774\n",
      "Mean value obs: -13.110290475500955\n",
      "STD obs: 0.5042344236353911\n",
      "\n",
      "Number of Samples: 19000\n",
      "Mean value obs: -13.12099922010142\n",
      "STD obs: 0.843674753956264\n",
      "Mean value obs: -13.123893145109477\n",
      "STD obs: 0.6813365615887835\n",
      "\n",
      "Number of Samples: 20000\n",
      "Mean value obs: -13.11730499301551\n",
      "STD obs: 0.7857618983365021\n",
      "Mean value obs: -13.11839199988842\n",
      "STD obs: 0.6031459083604087\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 21):\n",
    "    t_predictions = bnn_trained.predict(t_grp_descrp, t_logic_reduce, num_samples=1000*n)\n",
    "\n",
    "    print('\\nNumber of Samples: {}'.format(n*1000))\n",
    "    print('Mean value obs: {}'.format(torch.mean(t_predictions['obs'][:,1], 0).numpy()))\n",
    "    print('STD obs: {}'.format(torch.std(t_predictions['obs'][:,1], 0).numpy()))\n",
    "\n",
    "    print('Mean value obs: {}'.format(torch.mean(t_predictions['_RETURN'][:,1], 0).numpy()))\n",
    "    print('STD obs: {}'.format(torch.std(t_predictions['_RETURN'][:,1], 0).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.26069700348863"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NetAtom(tin.networks_param[\"input_size\"], tin.networks_param[\"hidden_size\"],\n",
    "\t\t\t    tin.sys_species, tin.networks_param[\"activations\"], tin.alpha, device)\n",
    "\n",
    "bnn = BayesianNeuralNetwork(net)\n",
    "\n",
    "bnn.train(grouped_train_loader, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.16938014 4.00191566 3.62253959 3.38525584 3.73111089 5.38229097\n",
      " 3.99725794 3.88566738 3.84382997 3.23328602 3.96624393 3.73362059\n",
      " 3.96680378 3.30055488 3.45105927 3.91641656 4.82355363 3.63048142\n",
      " 3.68462159 3.70996793]\n"
     ]
    }
   ],
   "source": [
    "v_predictions = bnn.predict(v_grp_descrp, v_logic_reduce, num_samples=800)\n",
    "t_predictions = bnn.predict(t_grp_descrp, t_logic_reduce, num_samples=800)\n",
    "\n",
    "t_std = torch.std(t_predictions['obs'], 0).numpy()\n",
    "v_std = torch.std(v_predictions['obs'], 0).numpy()\n",
    "\n",
    "print(t_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1693801428269697 9.773642700932248\n",
      "4.001915662242675 9.779298349244522\n",
      "3.622539585439741 9.80644698051442\n",
      "3.3852558423642294 9.798003385848006\n",
      "3.7311108892894254 9.773315119444723\n",
      "5.382290968723228 9.814592696415906\n",
      "3.997257939634929 9.800738971808016\n",
      "3.8856673825914996 9.852944005258575\n",
      "3.843829974772948 9.807626130796052\n",
      "3.2332860241550763 9.807246663172952\n",
      "3.9662439312957245 9.747403295044112\n",
      "3.7336205897704318 9.804015795756394\n",
      "3.9668037833987944 9.817125330597326\n",
      "3.300554878018853 9.833374874343239\n",
      "3.451059269631064 9.824919554464017\n",
      "3.9164165615030466 9.748966140591872\n",
      "4.823553628776083 9.803109060760892\n",
      "3.630481415706456 9.806264675505801\n",
      "3.6846215864646124 9.769581502733605\n"
     ]
    }
   ],
   "source": [
    "for i, std_i in zip(test_indices, t_std): \n",
    "    new_indinces = training_indices + [i]\n",
    "    new_grouped_train_loader, _, _ = get_sets_from_indices(new_indinces, test_indices, valid_indices, list_structures_energy)\n",
    "    \n",
    "    bnn_trained = copy.deepcopy(bnn)\n",
    "\n",
    "    bnn_trained.train(new_grouped_train_loader, 10000)\n",
    "\n",
    "    v_predictions = bnn_trained.predict(v_grp_descrp, v_logic_reduce, num_samples=800)\n",
    "    v_std = torch.std(v_predictions['obs'], 0).numpy()\n",
    "\n",
    "    print(std_i, np.mean(v_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted_std_test[-5:]\n",
    "guided_choice = [test_indices[x] for x in sorted_std_test[-5:]] + training_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_grp_descrp, v_grp_energy, v_logic_reduce, v_grp_N_atoms = get_train_test(grouped_valid_loader) \n",
    "t_grp_descrp, t_grp_energy, t_logic_reduce, t_grp_N_atoms = get_train_test(grouped_test_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH LOSS 0001] loss: 17.0463\n",
      "[EPOCH LOSS 0101] loss: 7.7749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH LOSS 0201] loss: 4.8907\n",
      "[EPOCH LOSS 0301] loss: 4.6197\n",
      "[EPOCH LOSS 0401] loss: 4.5332\n",
      "[EPOCH LOSS 0501] loss: 4.4094\n",
      "[EPOCH LOSS 0601] loss: 4.3450\n",
      "[EPOCH LOSS 0701] loss: 4.2393\n",
      "[EPOCH LOSS 0801] loss: 4.4226\n",
      "[EPOCH LOSS 0901] loss: 4.4196\n",
      "[EPOCH LOSS 1001] loss: 4.3615\n",
      "[EPOCH LOSS 1101] loss: 4.0253\n",
      "[EPOCH LOSS 1201] loss: 3.9867\n",
      "[EPOCH LOSS 1301] loss: 4.0377\n",
      "[EPOCH LOSS 1401] loss: 3.9345\n",
      "[EPOCH LOSS 1501] loss: 3.7175\n",
      "[EPOCH LOSS 1601] loss: 3.8345\n",
      "[EPOCH LOSS 1701] loss: 3.8607\n",
      "[EPOCH LOSS 1801] loss: 3.6053\n",
      "[EPOCH LOSS 1901] loss: 3.7277\n",
      "[EPOCH LOSS 2001] loss: 3.4045\n",
      "[EPOCH LOSS 2101] loss: 3.8286\n",
      "[EPOCH LOSS 2201] loss: 3.6320\n",
      "[EPOCH LOSS 2301] loss: 3.4881\n",
      "[EPOCH LOSS 2401] loss: 5.6838\n",
      "[EPOCH LOSS 2501] loss: 3.3327\n",
      "[EPOCH LOSS 2601] loss: 3.4428\n",
      "[EPOCH LOSS 2701] loss: 3.4388\n",
      "[EPOCH LOSS 2801] loss: 3.4777\n",
      "[EPOCH LOSS 2901] loss: 3.8206\n",
      "[EPOCH LOSS 3001] loss: 4.6018\n",
      "[EPOCH LOSS 3101] loss: 3.3301\n",
      "[EPOCH LOSS 3201] loss: 3.3076\n",
      "[EPOCH LOSS 3301] loss: 3.1087\n",
      "[EPOCH LOSS 3401] loss: 3.3273\n",
      "[EPOCH LOSS 3501] loss: 3.2917\n",
      "[EPOCH LOSS 3601] loss: 3.2698\n",
      "[EPOCH LOSS 3701] loss: 4.2968\n",
      "[EPOCH LOSS 3801] loss: 3.1173\n",
      "[EPOCH LOSS 3901] loss: 3.2551\n",
      "[EPOCH LOSS 4001] loss: 3.1753\n",
      "[EPOCH LOSS 4101] loss: 3.1771\n",
      "[EPOCH LOSS 4201] loss: 3.4052\n",
      "[EPOCH LOSS 4301] loss: 3.3276\n",
      "[EPOCH LOSS 4401] loss: 3.2039\n",
      "[EPOCH LOSS 4501] loss: 3.6461\n",
      "[EPOCH LOSS 4601] loss: 3.3684\n",
      "[EPOCH LOSS 4701] loss: 3.2107\n",
      "[EPOCH LOSS 4801] loss: 3.4135\n",
      "[EPOCH LOSS 4901] loss: 3.6214\n",
      "[EPOCH LOSS 5001] loss: 4.2533\n",
      "[EPOCH LOSS 5101] loss: 3.0068\n",
      "[EPOCH LOSS 5201] loss: 2.9845\n",
      "[EPOCH LOSS 5301] loss: 3.0340\n",
      "[EPOCH LOSS 5401] loss: 3.2496\n",
      "[EPOCH LOSS 5501] loss: 3.3011\n",
      "[EPOCH LOSS 5601] loss: 3.2173\n",
      "[EPOCH LOSS 5701] loss: 3.3095\n",
      "[EPOCH LOSS 5801] loss: 3.0918\n",
      "[EPOCH LOSS 5901] loss: 2.9528\n",
      "[EPOCH LOSS 6001] loss: 3.0916\n",
      "[EPOCH LOSS 6101] loss: 3.1805\n",
      "[EPOCH LOSS 6201] loss: 3.2949\n",
      "[EPOCH LOSS 6301] loss: 3.2675\n",
      "[EPOCH LOSS 6401] loss: 2.9055\n",
      "[EPOCH LOSS 6501] loss: 3.0362\n",
      "[EPOCH LOSS 6601] loss: 3.1052\n",
      "[EPOCH LOSS 6701] loss: 2.8435\n",
      "[EPOCH LOSS 6801] loss: 3.2800\n",
      "[EPOCH LOSS 6901] loss: 2.8115\n",
      "[EPOCH LOSS 7001] loss: 2.9215\n",
      "[EPOCH LOSS 7101] loss: 3.0506\n",
      "[EPOCH LOSS 7201] loss: 2.9800\n",
      "[EPOCH LOSS 7301] loss: 2.9374\n",
      "[EPOCH LOSS 7401] loss: 2.8245\n",
      "[EPOCH LOSS 7501] loss: 3.8412\n",
      "[EPOCH LOSS 7601] loss: 3.0011\n",
      "[EPOCH LOSS 7701] loss: 2.9470\n",
      "[EPOCH LOSS 7801] loss: 2.9350\n",
      "[EPOCH LOSS 7901] loss: 2.8252\n",
      "[EPOCH LOSS 8001] loss: 2.8399\n",
      "[EPOCH LOSS 8101] loss: 2.9436\n",
      "[EPOCH LOSS 8201] loss: 3.1011\n",
      "[EPOCH LOSS 8301] loss: 3.0731\n",
      "[EPOCH LOSS 8401] loss: 3.3444\n",
      "[EPOCH LOSS 8501] loss: 2.7619\n",
      "[EPOCH LOSS 8601] loss: 2.8350\n",
      "[EPOCH LOSS 8701] loss: 3.4899\n",
      "[EPOCH LOSS 8801] loss: 3.0383\n",
      "[EPOCH LOSS 8901] loss: 2.7768\n",
      "[EPOCH LOSS 9001] loss: 3.1811\n",
      "[EPOCH LOSS 9101] loss: 3.1085\n",
      "[EPOCH LOSS 9201] loss: 2.7792\n",
      "[EPOCH LOSS 9301] loss: 3.3457\n",
      "[EPOCH LOSS 9401] loss: 2.8036\n",
      "[EPOCH LOSS 9501] loss: 3.3059\n",
      "[EPOCH LOSS 9601] loss: 3.1518\n",
      "[EPOCH LOSS 9701] loss: 2.8038\n",
      "[EPOCH LOSS 9801] loss: 2.8366\n",
      "[EPOCH LOSS 9901] loss: 2.6729\n",
      "[EPOCH LOSS 10001] loss: 2.8240\n",
      "[EPOCH LOSS 10101] loss: 2.8968\n",
      "[EPOCH LOSS 10201] loss: 2.8053\n",
      "[EPOCH LOSS 10301] loss: 2.8304\n",
      "[EPOCH LOSS 10401] loss: 2.9948\n",
      "[EPOCH LOSS 10501] loss: 2.8806\n",
      "[EPOCH LOSS 10601] loss: 2.7395\n",
      "[EPOCH LOSS 10701] loss: 2.8695\n",
      "[EPOCH LOSS 10801] loss: 3.6623\n",
      "[EPOCH LOSS 10901] loss: 2.8181\n",
      "[EPOCH LOSS 11001] loss: 3.5150\n",
      "[EPOCH LOSS 11101] loss: 2.9565\n",
      "[EPOCH LOSS 11201] loss: 3.1327\n",
      "[EPOCH LOSS 11301] loss: 2.9401\n",
      "[EPOCH LOSS 11401] loss: 3.4545\n",
      "[EPOCH LOSS 11501] loss: 3.1900\n",
      "[EPOCH LOSS 11601] loss: 2.9973\n",
      "[EPOCH LOSS 11701] loss: 3.3428\n",
      "[EPOCH LOSS 11801] loss: 2.6431\n",
      "[EPOCH LOSS 11901] loss: 2.9594\n",
      "[EPOCH LOSS 12001] loss: 2.6508\n",
      "[EPOCH LOSS 12101] loss: 2.6864\n",
      "[EPOCH LOSS 12201] loss: 2.8804\n",
      "[EPOCH LOSS 12301] loss: 2.8611\n",
      "[EPOCH LOSS 12401] loss: 2.8195\n",
      "[EPOCH LOSS 12501] loss: 3.0907\n",
      "[EPOCH LOSS 12601] loss: 2.8736\n",
      "[EPOCH LOSS 12701] loss: 2.8131\n",
      "[EPOCH LOSS 12801] loss: 2.9338\n",
      "[EPOCH LOSS 12901] loss: 2.8642\n",
      "[EPOCH LOSS 13001] loss: 2.7703\n",
      "[EPOCH LOSS 13101] loss: 3.4089\n",
      "[EPOCH LOSS 13201] loss: 2.7591\n",
      "[EPOCH LOSS 13301] loss: 2.7565\n",
      "[EPOCH LOSS 13401] loss: 2.7598\n",
      "[EPOCH LOSS 13501] loss: 2.9395\n",
      "[EPOCH LOSS 13601] loss: 3.0190\n",
      "[EPOCH LOSS 13701] loss: 2.6675\n",
      "[EPOCH LOSS 13801] loss: 2.5706\n",
      "[EPOCH LOSS 13901] loss: 3.3403\n",
      "[EPOCH LOSS 14001] loss: 2.7353\n",
      "[EPOCH LOSS 14101] loss: 2.6559\n",
      "[EPOCH LOSS 14201] loss: 3.0447\n",
      "[EPOCH LOSS 14301] loss: 2.8185\n",
      "[EPOCH LOSS 14401] loss: 2.9608\n",
      "[EPOCH LOSS 14501] loss: 3.3235\n",
      "[EPOCH LOSS 14601] loss: 3.0016\n",
      "[EPOCH LOSS 14701] loss: 2.6020\n",
      "[EPOCH LOSS 14801] loss: 2.5511\n",
      "[EPOCH LOSS 14901] loss: 2.7512\n",
      "[EPOCH LOSS 15001] loss: 3.0947\n",
      "[EPOCH LOSS 15101] loss: 2.6293\n",
      "[EPOCH LOSS 15201] loss: 3.7210\n",
      "[EPOCH LOSS 15301] loss: 2.8284\n",
      "[EPOCH LOSS 15401] loss: 3.0535\n",
      "[EPOCH LOSS 15501] loss: 2.6569\n",
      "[EPOCH LOSS 15601] loss: 2.7281\n",
      "[EPOCH LOSS 15701] loss: 2.7366\n",
      "[EPOCH LOSS 15801] loss: 2.9338\n",
      "[EPOCH LOSS 15901] loss: 2.6890\n",
      "[EPOCH LOSS 16001] loss: 2.7430\n",
      "[EPOCH LOSS 16101] loss: 2.6025\n",
      "[EPOCH LOSS 16201] loss: 3.1526\n",
      "[EPOCH LOSS 16301] loss: 2.7460\n",
      "[EPOCH LOSS 16401] loss: 2.6240\n",
      "[EPOCH LOSS 16501] loss: 2.9831\n",
      "[EPOCH LOSS 16601] loss: 2.8914\n",
      "[EPOCH LOSS 16701] loss: 2.7923\n",
      "[EPOCH LOSS 16801] loss: 2.5767\n",
      "[EPOCH LOSS 16901] loss: 2.8432\n",
      "[EPOCH LOSS 17001] loss: 2.8138\n",
      "[EPOCH LOSS 17101] loss: 2.9044\n",
      "[EPOCH LOSS 17201] loss: 2.7474\n",
      "[EPOCH LOSS 17301] loss: 2.9061\n",
      "[EPOCH LOSS 17401] loss: 2.7886\n",
      "[EPOCH LOSS 17501] loss: 3.0691\n",
      "[EPOCH LOSS 17601] loss: 2.6960\n",
      "[EPOCH LOSS 17701] loss: 2.8455\n",
      "[EPOCH LOSS 17801] loss: 2.9316\n",
      "[EPOCH LOSS 17901] loss: 2.7824\n",
      "[EPOCH LOSS 18001] loss: 3.0034\n",
      "[EPOCH LOSS 18101] loss: 2.7206\n",
      "[EPOCH LOSS 18201] loss: 2.7529\n",
      "[EPOCH LOSS 18301] loss: 2.7762\n",
      "[EPOCH LOSS 18401] loss: 2.7486\n",
      "[EPOCH LOSS 18501] loss: 2.8874\n",
      "[EPOCH LOSS 18601] loss: 2.8001\n",
      "[EPOCH LOSS 18701] loss: 2.8401\n",
      "[EPOCH LOSS 18801] loss: 3.3408\n",
      "[EPOCH LOSS 18901] loss: 2.7083\n",
      "[EPOCH LOSS 19001] loss: 3.1120\n",
      "[EPOCH LOSS 19101] loss: 2.5851\n",
      "[EPOCH LOSS 19201] loss: 2.5606\n",
      "[EPOCH LOSS 19301] loss: 3.0634\n",
      "[EPOCH LOSS 19401] loss: 2.7674\n",
      "[EPOCH LOSS 19501] loss: 2.6202\n",
      "[EPOCH LOSS 19601] loss: 3.0284\n",
      "[EPOCH LOSS 19701] loss: 2.7424\n",
      "[EPOCH LOSS 19801] loss: 2.8844\n",
      "[EPOCH LOSS 19901] loss: 2.5561\n",
      "[EPOCH LOSS 20001] loss: 2.9602\n",
      "[EPOCH LOSS 20101] loss: 2.5325\n",
      "[EPOCH LOSS 20201] loss: 2.7006\n",
      "[EPOCH LOSS 20301] loss: 2.9349\n",
      "[EPOCH LOSS 20401] loss: 2.6194\n",
      "[EPOCH LOSS 20501] loss: 2.5464\n",
      "[EPOCH LOSS 20601] loss: 2.7099\n",
      "[EPOCH LOSS 20701] loss: 2.8146\n",
      "[EPOCH LOSS 20801] loss: 2.6756\n",
      "[EPOCH LOSS 20901] loss: 2.6862\n",
      "[EPOCH LOSS 21001] loss: 2.5855\n",
      "[EPOCH LOSS 21101] loss: 2.7761\n",
      "[EPOCH LOSS 21201] loss: 3.0831\n",
      "[EPOCH LOSS 21301] loss: 3.0202\n",
      "[EPOCH LOSS 21401] loss: 2.6824\n",
      "[EPOCH LOSS 21501] loss: 2.7130\n",
      "[EPOCH LOSS 21601] loss: 3.3357\n",
      "[EPOCH LOSS 21701] loss: 2.5807\n",
      "[EPOCH LOSS 21801] loss: 2.5677\n",
      "[EPOCH LOSS 21901] loss: 2.5208\n",
      "[EPOCH LOSS 22001] loss: 3.0586\n",
      "[EPOCH LOSS 22101] loss: 3.5697\n",
      "[EPOCH LOSS 22201] loss: 2.5503\n",
      "[EPOCH LOSS 22301] loss: 3.3007\n",
      "[EPOCH LOSS 22401] loss: 2.6907\n",
      "[EPOCH LOSS 22501] loss: 2.5500\n",
      "[EPOCH LOSS 22601] loss: 2.6266\n",
      "[EPOCH LOSS 22701] loss: 3.0534\n",
      "[EPOCH LOSS 22801] loss: 2.7116\n",
      "[EPOCH LOSS 22901] loss: 2.7744\n",
      "[EPOCH LOSS 23001] loss: 3.4774\n",
      "[EPOCH LOSS 23101] loss: 2.5831\n",
      "[EPOCH LOSS 23201] loss: 2.7330\n",
      "[EPOCH LOSS 23301] loss: 2.6117\n",
      "[EPOCH LOSS 23401] loss: 2.6412\n",
      "[EPOCH LOSS 23501] loss: 2.4464\n",
      "[EPOCH LOSS 23601] loss: 2.7174\n",
      "[EPOCH LOSS 23701] loss: 2.6529\n",
      "[EPOCH LOSS 23801] loss: 2.6225\n",
      "[EPOCH LOSS 23901] loss: 2.5594\n",
      "[EPOCH LOSS 24001] loss: 2.6144\n",
      "[EPOCH LOSS 24101] loss: 2.4783\n",
      "[EPOCH LOSS 24201] loss: 2.6002\n",
      "[EPOCH LOSS 24301] loss: 2.8428\n",
      "[EPOCH LOSS 24401] loss: 2.7071\n",
      "[EPOCH LOSS 24501] loss: 2.8018\n",
      "[EPOCH LOSS 24601] loss: 2.5991\n",
      "[EPOCH LOSS 24701] loss: 2.4636\n",
      "[EPOCH LOSS 24801] loss: 3.1038\n",
      "[EPOCH LOSS 24901] loss: 3.4188\n",
      "[EPOCH LOSS 25001] loss: 2.6662\n",
      "[EPOCH LOSS 25101] loss: 2.6540\n",
      "[EPOCH LOSS 25201] loss: 2.4560\n",
      "[EPOCH LOSS 25301] loss: 2.7596\n",
      "[EPOCH LOSS 25401] loss: 2.7621\n",
      "[EPOCH LOSS 25501] loss: 2.7365\n",
      "[EPOCH LOSS 25601] loss: 2.8809\n",
      "[EPOCH LOSS 25701] loss: 2.8767\n",
      "[EPOCH LOSS 25801] loss: 2.9062\n",
      "[EPOCH LOSS 25901] loss: 2.6027\n",
      "[EPOCH LOSS 26001] loss: 2.5131\n",
      "[EPOCH LOSS 26101] loss: 2.7403\n",
      "[EPOCH LOSS 26201] loss: 2.5168\n",
      "[EPOCH LOSS 26301] loss: 2.6288\n",
      "[EPOCH LOSS 26401] loss: 2.5662\n",
      "[EPOCH LOSS 26501] loss: 2.7349\n",
      "[EPOCH LOSS 26601] loss: 2.6775\n",
      "[EPOCH LOSS 26701] loss: 2.5874\n",
      "[EPOCH LOSS 26801] loss: 2.6203\n",
      "[EPOCH LOSS 26901] loss: 2.7426\n",
      "[EPOCH LOSS 27001] loss: 2.5968\n",
      "[EPOCH LOSS 27101] loss: 2.8421\n",
      "[EPOCH LOSS 27201] loss: 2.7257\n",
      "[EPOCH LOSS 27301] loss: 2.6598\n",
      "[EPOCH LOSS 27401] loss: 2.8862\n",
      "[EPOCH LOSS 27501] loss: 2.7551\n",
      "[EPOCH LOSS 27601] loss: 2.5983\n",
      "[EPOCH LOSS 27701] loss: 4.2375\n",
      "[EPOCH LOSS 27801] loss: 2.8108\n",
      "[EPOCH LOSS 27901] loss: 2.9162\n",
      "[EPOCH LOSS 28001] loss: 3.1373\n",
      "[EPOCH LOSS 28101] loss: 2.5447\n",
      "[EPOCH LOSS 28201] loss: 2.6943\n",
      "[EPOCH LOSS 28301] loss: 2.5310\n",
      "[EPOCH LOSS 28401] loss: 2.9897\n",
      "[EPOCH LOSS 28501] loss: 2.6341\n",
      "[EPOCH LOSS 28601] loss: 2.7044\n",
      "[EPOCH LOSS 28701] loss: 2.5643\n",
      "[EPOCH LOSS 28801] loss: 2.7141\n",
      "[EPOCH LOSS 28901] loss: 2.6226\n",
      "[EPOCH LOSS 29001] loss: 2.7811\n",
      "[EPOCH LOSS 29101] loss: 2.5852\n",
      "[EPOCH LOSS 29201] loss: 2.7600\n",
      "[EPOCH LOSS 29301] loss: 2.5891\n",
      "[EPOCH LOSS 29401] loss: 2.7479\n",
      "[EPOCH LOSS 29501] loss: 2.4655\n",
      "[EPOCH LOSS 29601] loss: 2.7703\n",
      "[EPOCH LOSS 29701] loss: 2.9907\n",
      "[EPOCH LOSS 29801] loss: 2.6162\n",
      "[EPOCH LOSS 29901] loss: 2.5729\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "num_steps = 30000\n",
    "initial_lr = 0.001\n",
    "gamma = 0.1  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "optim = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "svi = SVI(bnn.model, guide, optim, loss=TraceMeanField_ELBO())\n",
    "\n",
    "for j in range(num_steps):\n",
    "    loss = 0\n",
    "    for data_batch in grouped_train_loader:\n",
    "        grp_descrp, grp_energy, logic_reduce = data_batch[0][10], data_batch[0][11], data_batch[0][12]\n",
    "        grp_descrp[0] = grp_descrp[0].float()\n",
    "        grp_descrp[1] = grp_descrp[1].float()\n",
    "\n",
    "        logic_reduce[0] = logic_reduce[0].float()\n",
    "        logic_reduce[1] = logic_reduce[1].float()\n",
    "        loss += svi.step(grp_descrp, logic_reduce, grp_energy) \n",
    "    if j % 100 == 0:\n",
    "        print(\"[EPOCH LOSS %04d] loss: %.4f\" % (j + 1, loss / len(logic_reduce[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grouped_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = Predictive(bnn.model, guide=guide, num_samples=8000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "\n",
    "v_grp_descrp, v_grp_energy, v_logic_reduce, v_grp_N_atoms = get_train_test(grouped_valid_loader) \n",
    "t_grp_descrp, t_grp_energy, t_logic_reduce, t_grp_N_atoms = get_train_test(grouped_test_loader)    \n",
    "\n",
    "v_samples = predictive(v_grp_descrp, v_logic_reduce)\n",
    "t_samples = predictive(t_grp_descrp, t_logic_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation Test Set 2.3848565685704215\n",
      "Standard Deviation Validation Set 2.6130075268125155\n"
     ]
    }
   ],
   "source": [
    "v_std = torch.std(v_samples['obs'], 0).numpy()\n",
    "t_std = torch.std(t_samples['obs'], 0).numpy()\n",
    "\n",
    "print('Standard Deviation Test Set {}'.format(np.mean(t_std)))\n",
    "print('Standard Deviation Validation Set {}'.format(np.mean(v_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_std_test = np.argsort(torch.std(t_samples['obs'], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_choice = [test_indices[x] for x in sorted_std_test[-5:]] + training_indices\n",
    "shuffle_test = np.random.choice(len(test_indices), len(test_indices), replace=False)\n",
    "random_choice = [test_indices[x] for x in shuffle_test[-5:]] + training_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4415, 2.5250, 2.5796, 2.7677, 3.6075], dtype=torch.float64)\n",
      "tensor([2.7677, 2.3808, 2.1901, 2.2747, 2.1727], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.std(t_samples['obs'][:,sorted_std_test[-5:]], dim=0))\n",
    "print(torch.std(t_samples['obs'][:,shuffle_test[-5:]], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUIDED CHOICE\n",
    "\n",
    "net = NetAtom(tin.networks_param[\"input_size\"], tin.networks_param[\"hidden_size\"],\n",
    "\t\t\t    tin.sys_species, tin.networks_param[\"activations\"], tin.alpha, device)\n",
    "\n",
    "guided_model = copy.deepcopy(bnn)\n",
    "guided_guide = copy.deepcopy(guide)\n",
    "\n",
    "list_structures_energy, list_structures_forces, list_removed, max_nnb, tin = read_list_structures(tin)\n",
    "G_grouped_train_loader, G_grouped_test_loader, G_grouped_valid_loader = get_sets_from_indices(guided_choice, test_indices, valid_indices, list_structures_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH LOSS 0001] loss: 2.7224\n",
      "[EPOCH LOSS 0101] loss: 3.0565\n",
      "[EPOCH LOSS 0201] loss: 2.6703\n",
      "[EPOCH LOSS 0301] loss: 2.7540\n",
      "[EPOCH LOSS 0401] loss: 2.6830\n",
      "[EPOCH LOSS 0501] loss: 2.5228\n",
      "[EPOCH LOSS 0601] loss: 2.6069\n",
      "[EPOCH LOSS 0701] loss: 2.4956\n",
      "[EPOCH LOSS 0801] loss: 2.4480\n",
      "[EPOCH LOSS 0901] loss: 2.5127\n",
      "[EPOCH LOSS 1001] loss: 2.4854\n",
      "[EPOCH LOSS 1101] loss: 2.6648\n",
      "[EPOCH LOSS 1201] loss: 2.6071\n",
      "[EPOCH LOSS 1301] loss: 2.4373\n",
      "[EPOCH LOSS 1401] loss: 2.5080\n",
      "[EPOCH LOSS 1501] loss: 2.6573\n",
      "[EPOCH LOSS 1601] loss: 2.5926\n",
      "[EPOCH LOSS 1701] loss: 2.6522\n",
      "[EPOCH LOSS 1801] loss: 2.5841\n",
      "[EPOCH LOSS 1901] loss: 2.5841\n",
      "[EPOCH LOSS 2001] loss: 2.8068\n",
      "[EPOCH LOSS 2101] loss: 2.9259\n",
      "[EPOCH LOSS 2201] loss: 2.5486\n",
      "[EPOCH LOSS 2301] loss: 2.9848\n",
      "[EPOCH LOSS 2401] loss: 2.7530\n",
      "[EPOCH LOSS 2501] loss: 2.6728\n",
      "[EPOCH LOSS 2601] loss: 2.3884\n",
      "[EPOCH LOSS 2701] loss: 2.7636\n",
      "[EPOCH LOSS 2801] loss: 2.7813\n",
      "[EPOCH LOSS 2901] loss: 2.6957\n",
      "[EPOCH LOSS 3001] loss: 2.7482\n",
      "[EPOCH LOSS 3101] loss: 2.5000\n",
      "[EPOCH LOSS 3201] loss: 2.5240\n",
      "[EPOCH LOSS 3301] loss: 2.5883\n",
      "[EPOCH LOSS 3401] loss: 2.8182\n",
      "[EPOCH LOSS 3501] loss: 2.6452\n",
      "[EPOCH LOSS 3601] loss: 2.4040\n",
      "[EPOCH LOSS 3701] loss: 2.8555\n",
      "[EPOCH LOSS 3801] loss: 2.4231\n",
      "[EPOCH LOSS 3901] loss: 2.4675\n",
      "[EPOCH LOSS 4001] loss: 2.5575\n",
      "[EPOCH LOSS 4101] loss: 2.3543\n",
      "[EPOCH LOSS 4201] loss: 2.6594\n",
      "[EPOCH LOSS 4301] loss: 2.4053\n",
      "[EPOCH LOSS 4401] loss: 2.6217\n",
      "[EPOCH LOSS 4501] loss: 2.8153\n",
      "[EPOCH LOSS 4601] loss: 2.4665\n",
      "[EPOCH LOSS 4701] loss: 2.6425\n",
      "[EPOCH LOSS 4801] loss: 2.4920\n",
      "[EPOCH LOSS 4901] loss: 2.4466\n",
      "[EPOCH LOSS 5001] loss: 2.5130\n",
      "[EPOCH LOSS 5101] loss: 2.7563\n",
      "[EPOCH LOSS 5201] loss: 2.7467\n",
      "[EPOCH LOSS 5301] loss: 2.5006\n",
      "[EPOCH LOSS 5401] loss: 2.4422\n",
      "[EPOCH LOSS 5501] loss: 2.7691\n",
      "[EPOCH LOSS 5601] loss: 2.6288\n",
      "[EPOCH LOSS 5701] loss: 2.5132\n",
      "[EPOCH LOSS 5801] loss: 2.5331\n",
      "[EPOCH LOSS 5901] loss: 2.6330\n",
      "[EPOCH LOSS 6001] loss: 2.4672\n",
      "[EPOCH LOSS 6101] loss: 2.4668\n",
      "[EPOCH LOSS 6201] loss: 2.4135\n",
      "[EPOCH LOSS 6301] loss: 2.5855\n",
      "[EPOCH LOSS 6401] loss: 2.3983\n",
      "[EPOCH LOSS 6501] loss: 2.4712\n",
      "[EPOCH LOSS 6601] loss: 3.0631\n",
      "[EPOCH LOSS 6701] loss: 2.5982\n",
      "[EPOCH LOSS 6801] loss: 2.6616\n",
      "[EPOCH LOSS 6901] loss: 2.4644\n",
      "[EPOCH LOSS 7001] loss: 2.4552\n",
      "[EPOCH LOSS 7101] loss: 2.4566\n",
      "[EPOCH LOSS 7201] loss: 2.6393\n",
      "[EPOCH LOSS 7301] loss: 2.6170\n",
      "[EPOCH LOSS 7401] loss: 2.9291\n",
      "[EPOCH LOSS 7501] loss: 2.4473\n",
      "[EPOCH LOSS 7601] loss: 2.4407\n",
      "[EPOCH LOSS 7701] loss: 2.5736\n",
      "[EPOCH LOSS 7801] loss: 2.3697\n",
      "[EPOCH LOSS 7901] loss: 2.4170\n",
      "[EPOCH LOSS 8001] loss: 2.5836\n",
      "[EPOCH LOSS 8101] loss: 2.3435\n",
      "[EPOCH LOSS 8201] loss: 2.4480\n",
      "[EPOCH LOSS 8301] loss: 2.5637\n",
      "[EPOCH LOSS 8401] loss: 2.7214\n",
      "[EPOCH LOSS 8501] loss: 2.4106\n",
      "[EPOCH LOSS 8601] loss: 2.4235\n",
      "[EPOCH LOSS 8701] loss: 2.4126\n",
      "[EPOCH LOSS 8801] loss: 2.4918\n",
      "[EPOCH LOSS 8901] loss: 3.1734\n",
      "[EPOCH LOSS 9001] loss: 2.4835\n",
      "[EPOCH LOSS 9101] loss: 2.8049\n",
      "[EPOCH LOSS 9201] loss: 2.4682\n",
      "[EPOCH LOSS 9301] loss: 2.4342\n",
      "[EPOCH LOSS 9401] loss: 2.4511\n",
      "[EPOCH LOSS 9501] loss: 2.3452\n",
      "[EPOCH LOSS 9601] loss: 2.5427\n",
      "[EPOCH LOSS 9701] loss: 2.5572\n",
      "[EPOCH LOSS 9801] loss: 2.3570\n",
      "[EPOCH LOSS 9901] loss: 2.3870\n",
      "[EPOCH LOSS 10001] loss: 2.3736\n",
      "[EPOCH LOSS 10101] loss: 2.5364\n",
      "[EPOCH LOSS 10201] loss: 2.5531\n",
      "[EPOCH LOSS 10301] loss: 2.7407\n",
      "[EPOCH LOSS 10401] loss: 2.4767\n",
      "[EPOCH LOSS 10501] loss: 2.6470\n",
      "[EPOCH LOSS 10601] loss: 2.4766\n",
      "[EPOCH LOSS 10701] loss: 2.4994\n",
      "[EPOCH LOSS 10801] loss: 2.5334\n",
      "[EPOCH LOSS 10901] loss: 2.3713\n",
      "[EPOCH LOSS 11001] loss: 2.5361\n",
      "[EPOCH LOSS 11101] loss: 2.8323\n",
      "[EPOCH LOSS 11201] loss: 2.4680\n",
      "[EPOCH LOSS 11301] loss: 2.5316\n",
      "[EPOCH LOSS 11401] loss: 2.3569\n",
      "[EPOCH LOSS 11501] loss: 2.5678\n",
      "[EPOCH LOSS 11601] loss: 2.7722\n",
      "[EPOCH LOSS 11701] loss: 2.5446\n",
      "[EPOCH LOSS 11801] loss: 2.4931\n",
      "[EPOCH LOSS 11901] loss: 2.4612\n",
      "[EPOCH LOSS 12001] loss: 2.4175\n",
      "[EPOCH LOSS 12101] loss: 2.4228\n",
      "[EPOCH LOSS 12201] loss: 2.4319\n",
      "[EPOCH LOSS 12301] loss: 2.4997\n",
      "[EPOCH LOSS 12401] loss: 2.3491\n",
      "[EPOCH LOSS 12501] loss: 2.3297\n",
      "[EPOCH LOSS 12601] loss: 2.4338\n",
      "[EPOCH LOSS 12701] loss: 2.6325\n",
      "[EPOCH LOSS 12801] loss: 2.4231\n",
      "[EPOCH LOSS 12901] loss: 2.3112\n",
      "[EPOCH LOSS 13001] loss: 2.3969\n",
      "[EPOCH LOSS 13101] loss: 2.5506\n",
      "[EPOCH LOSS 13201] loss: 2.4600\n",
      "[EPOCH LOSS 13301] loss: 2.2404\n",
      "[EPOCH LOSS 13401] loss: 2.6361\n",
      "[EPOCH LOSS 13501] loss: 2.5950\n",
      "[EPOCH LOSS 13601] loss: 2.3732\n",
      "[EPOCH LOSS 13701] loss: 2.4620\n",
      "[EPOCH LOSS 13801] loss: 2.4019\n",
      "[EPOCH LOSS 13901] loss: 2.3902\n",
      "[EPOCH LOSS 14001] loss: 2.3637\n",
      "[EPOCH LOSS 14101] loss: 2.3755\n",
      "[EPOCH LOSS 14201] loss: 2.3620\n",
      "[EPOCH LOSS 14301] loss: 2.5866\n",
      "[EPOCH LOSS 14401] loss: 2.3896\n",
      "[EPOCH LOSS 14501] loss: 2.4376\n",
      "[EPOCH LOSS 14601] loss: 2.3893\n",
      "[EPOCH LOSS 14701] loss: 2.4877\n",
      "[EPOCH LOSS 14801] loss: 2.8082\n",
      "[EPOCH LOSS 14901] loss: 2.6029\n",
      "[EPOCH LOSS 15001] loss: 2.5070\n",
      "[EPOCH LOSS 15101] loss: 2.3101\n",
      "[EPOCH LOSS 15201] loss: 2.5955\n",
      "[EPOCH LOSS 15301] loss: 2.2817\n",
      "[EPOCH LOSS 15401] loss: 2.6591\n",
      "[EPOCH LOSS 15501] loss: 2.6761\n",
      "[EPOCH LOSS 15601] loss: 2.5386\n",
      "[EPOCH LOSS 15701] loss: 2.2786\n",
      "[EPOCH LOSS 15801] loss: 2.5032\n",
      "[EPOCH LOSS 15901] loss: 2.7135\n",
      "[EPOCH LOSS 16001] loss: 2.6681\n",
      "[EPOCH LOSS 16101] loss: 2.3863\n",
      "[EPOCH LOSS 16201] loss: 2.5792\n",
      "[EPOCH LOSS 16301] loss: 2.2900\n",
      "[EPOCH LOSS 16401] loss: 2.5970\n",
      "[EPOCH LOSS 16501] loss: 2.3743\n",
      "[EPOCH LOSS 16601] loss: 2.4528\n",
      "[EPOCH LOSS 16701] loss: 3.1154\n",
      "[EPOCH LOSS 16801] loss: 2.3111\n",
      "[EPOCH LOSS 16901] loss: 2.4844\n",
      "[EPOCH LOSS 17001] loss: 2.4516\n",
      "[EPOCH LOSS 17101] loss: 2.3452\n",
      "[EPOCH LOSS 17201] loss: 2.5995\n",
      "[EPOCH LOSS 17301] loss: 2.5053\n",
      "[EPOCH LOSS 17401] loss: 2.4955\n",
      "[EPOCH LOSS 17501] loss: 2.3649\n",
      "[EPOCH LOSS 17601] loss: 2.2328\n",
      "[EPOCH LOSS 17701] loss: 3.2685\n",
      "[EPOCH LOSS 17801] loss: 2.3462\n",
      "[EPOCH LOSS 17901] loss: 3.2954\n",
      "[EPOCH LOSS 18001] loss: 2.2893\n",
      "[EPOCH LOSS 18101] loss: 2.3598\n",
      "[EPOCH LOSS 18201] loss: 2.2105\n",
      "[EPOCH LOSS 18301] loss: 2.5201\n",
      "[EPOCH LOSS 18401] loss: 2.4486\n",
      "[EPOCH LOSS 18501] loss: 2.2727\n",
      "[EPOCH LOSS 18601] loss: 2.3905\n",
      "[EPOCH LOSS 18701] loss: 2.6687\n",
      "[EPOCH LOSS 18801] loss: 2.4139\n",
      "[EPOCH LOSS 18901] loss: 2.4112\n",
      "[EPOCH LOSS 19001] loss: 2.6451\n",
      "[EPOCH LOSS 19101] loss: 2.4136\n",
      "[EPOCH LOSS 19201] loss: 2.4133\n",
      "[EPOCH LOSS 19301] loss: 2.5595\n",
      "[EPOCH LOSS 19401] loss: 2.3266\n",
      "[EPOCH LOSS 19501] loss: 2.2525\n",
      "[EPOCH LOSS 19601] loss: 2.4700\n",
      "[EPOCH LOSS 19701] loss: 2.2506\n",
      "[EPOCH LOSS 19801] loss: 2.2758\n",
      "[EPOCH LOSS 19901] loss: 2.3500\n",
      "[EPOCH LOSS 20001] loss: 2.4696\n",
      "[EPOCH LOSS 20101] loss: 2.3186\n",
      "[EPOCH LOSS 20201] loss: 2.4258\n",
      "[EPOCH LOSS 20301] loss: 2.6449\n",
      "[EPOCH LOSS 20401] loss: 2.5654\n",
      "[EPOCH LOSS 20501] loss: 2.3283\n",
      "[EPOCH LOSS 20601] loss: 2.5043\n",
      "[EPOCH LOSS 20701] loss: 2.6246\n",
      "[EPOCH LOSS 20801] loss: 2.2864\n",
      "[EPOCH LOSS 20901] loss: 2.2636\n",
      "[EPOCH LOSS 21001] loss: 2.5058\n",
      "[EPOCH LOSS 21101] loss: 2.4299\n",
      "[EPOCH LOSS 21201] loss: 2.3689\n",
      "[EPOCH LOSS 21301] loss: 2.2690\n",
      "[EPOCH LOSS 21401] loss: 2.4730\n",
      "[EPOCH LOSS 21501] loss: 2.3682\n",
      "[EPOCH LOSS 21601] loss: 2.3798\n",
      "[EPOCH LOSS 21701] loss: 2.3811\n",
      "[EPOCH LOSS 21801] loss: 2.5562\n",
      "[EPOCH LOSS 21901] loss: 2.3439\n",
      "[EPOCH LOSS 22001] loss: 2.3255\n",
      "[EPOCH LOSS 22101] loss: 2.4879\n",
      "[EPOCH LOSS 22201] loss: 2.2609\n",
      "[EPOCH LOSS 22301] loss: 2.6671\n",
      "[EPOCH LOSS 22401] loss: 2.5359\n",
      "[EPOCH LOSS 22501] loss: 2.5885\n",
      "[EPOCH LOSS 22601] loss: 2.5082\n",
      "[EPOCH LOSS 22701] loss: 2.5080\n",
      "[EPOCH LOSS 22801] loss: 2.9031\n",
      "[EPOCH LOSS 22901] loss: 2.5408\n",
      "[EPOCH LOSS 23001] loss: 2.5140\n",
      "[EPOCH LOSS 23101] loss: 2.4543\n",
      "[EPOCH LOSS 23201] loss: 2.4234\n",
      "[EPOCH LOSS 23301] loss: 2.3093\n",
      "[EPOCH LOSS 23401] loss: 2.4571\n",
      "[EPOCH LOSS 23501] loss: 2.4522\n",
      "[EPOCH LOSS 23601] loss: 2.3906\n",
      "[EPOCH LOSS 23701] loss: 2.4311\n",
      "[EPOCH LOSS 23801] loss: 2.3218\n",
      "[EPOCH LOSS 23901] loss: 2.7782\n",
      "[EPOCH LOSS 24001] loss: 2.3925\n",
      "[EPOCH LOSS 24101] loss: 2.3864\n",
      "[EPOCH LOSS 24201] loss: 2.4378\n",
      "[EPOCH LOSS 24301] loss: 2.2887\n",
      "[EPOCH LOSS 24401] loss: 2.4147\n",
      "[EPOCH LOSS 24501] loss: 2.4147\n",
      "[EPOCH LOSS 24601] loss: 2.5595\n",
      "[EPOCH LOSS 24701] loss: 2.3641\n",
      "[EPOCH LOSS 24801] loss: 2.3771\n",
      "[EPOCH LOSS 24901] loss: 2.4569\n",
      "[EPOCH LOSS 25001] loss: 2.4355\n",
      "[EPOCH LOSS 25101] loss: 2.5132\n",
      "[EPOCH LOSS 25201] loss: 2.3269\n",
      "[EPOCH LOSS 25301] loss: 2.8087\n",
      "[EPOCH LOSS 25401] loss: 2.2714\n",
      "[EPOCH LOSS 25501] loss: 2.5826\n",
      "[EPOCH LOSS 25601] loss: 2.3494\n",
      "[EPOCH LOSS 25701] loss: 2.4071\n",
      "[EPOCH LOSS 25801] loss: 2.4722\n",
      "[EPOCH LOSS 25901] loss: 2.2615\n",
      "[EPOCH LOSS 26001] loss: 2.3287\n",
      "[EPOCH LOSS 26101] loss: 2.6080\n",
      "[EPOCH LOSS 26201] loss: 2.8795\n",
      "[EPOCH LOSS 26301] loss: 2.3148\n",
      "[EPOCH LOSS 26401] loss: 2.4457\n",
      "[EPOCH LOSS 26501] loss: 2.3185\n",
      "[EPOCH LOSS 26601] loss: 2.2645\n",
      "[EPOCH LOSS 26701] loss: 2.5761\n",
      "[EPOCH LOSS 26801] loss: 2.3407\n",
      "[EPOCH LOSS 26901] loss: 2.3339\n",
      "[EPOCH LOSS 27001] loss: 2.2330\n",
      "[EPOCH LOSS 27101] loss: 2.3072\n",
      "[EPOCH LOSS 27201] loss: 2.3132\n",
      "[EPOCH LOSS 27301] loss: 2.3833\n",
      "[EPOCH LOSS 27401] loss: 2.4523\n",
      "[EPOCH LOSS 27501] loss: 2.3913\n",
      "[EPOCH LOSS 27601] loss: 2.4435\n",
      "[EPOCH LOSS 27701] loss: 2.2914\n",
      "[EPOCH LOSS 27801] loss: 2.3001\n",
      "[EPOCH LOSS 27901] loss: 2.5843\n",
      "[EPOCH LOSS 28001] loss: 2.3424\n",
      "[EPOCH LOSS 28101] loss: 2.4367\n",
      "[EPOCH LOSS 28201] loss: 2.3098\n",
      "[EPOCH LOSS 28301] loss: 2.4289\n",
      "[EPOCH LOSS 28401] loss: 2.3957\n",
      "[EPOCH LOSS 28501] loss: 2.4084\n",
      "[EPOCH LOSS 28601] loss: 2.4803\n",
      "[EPOCH LOSS 28701] loss: 2.3648\n",
      "[EPOCH LOSS 28801] loss: 2.3726\n",
      "[EPOCH LOSS 28901] loss: 2.4628\n",
      "[EPOCH LOSS 29001] loss: 2.3929\n",
      "[EPOCH LOSS 29101] loss: 2.5562\n",
      "[EPOCH LOSS 29201] loss: 2.4095\n",
      "[EPOCH LOSS 29301] loss: 2.4880\n",
      "[EPOCH LOSS 29401] loss: 2.4188\n",
      "[EPOCH LOSS 29501] loss: 2.5593\n",
      "[EPOCH LOSS 29601] loss: 2.6967\n",
      "[EPOCH LOSS 29701] loss: 2.3614\n",
      "[EPOCH LOSS 29801] loss: 2.3166\n",
      "[EPOCH LOSS 29901] loss: 2.7669\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "num_steps = 30000\n",
    "initial_lr = 0.001\n",
    "gamma = 0.1  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "optim = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "svi = SVI(guided_model.model, guided_guide, optim, loss=TraceMeanField_ELBO())\n",
    "\n",
    "for j in range(num_steps):\n",
    "    loss = 0\n",
    "    for data_batch in G_grouped_train_loader:\n",
    "        grp_descrp, grp_energy, logic_reduce = data_batch[0][10], data_batch[0][11], data_batch[0][12]\n",
    "        grp_descrp[0] = grp_descrp[0].float()\n",
    "        grp_descrp[1] = grp_descrp[1].float()\n",
    "\n",
    "        logic_reduce[0] = logic_reduce[0].float()\n",
    "        logic_reduce[1] = logic_reduce[1].float()\n",
    "        loss += svi.step(grp_descrp, logic_reduce, grp_energy) \n",
    "    if j % 100 == 0:\n",
    "        print(\"[EPOCH LOSS %04d] loss: %.4f\" % (j + 1, loss / len(logic_reduce[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD Validation Set before guided choice: 2.6130075268125155\n",
      "STD Validation Set after guided choice: 1.9240081625845111\n"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(guided_model.model, guide=guided_guide, num_samples=10000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "\n",
    "G_v_samples = predictive(v_grp_descrp, v_logic_reduce)\n",
    "G_v_std = torch.std(G_v_samples['obs'], 0).numpy()\n",
    "\n",
    "print('STD Validation Set before guided choice: {}'.format(np.mean(v_std)))\n",
    "print('STD Validation Set after guided choice: {}'.format(np.mean(G_v_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertanty guided active learning and random active learning model split\n",
    "net = NetAtom(tin.networks_param[\"input_size\"], tin.networks_param[\"hidden_size\"],\n",
    "\t\t\t    tin.sys_species, tin.networks_param[\"activations\"], tin.alpha, device)\n",
    "\n",
    "random_model = copy.deepcopy(bnn)\n",
    "random_guide = copy.deepcopy(guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM CHOICE\n",
    "list_structures_energy, list_structures_forces, list_removed, max_nnb, tin = read_list_structures(tin)\n",
    "R_grouped_train_loader,R_grouped_test_loader, R_grouped_valid_loader = get_sets_from_indices(random_choice, test_indices, valid_indices, list_structures_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH LOSS 0001] loss: 2.6842\n",
      "[EPOCH LOSS 0101] loss: 2.7201\n",
      "[EPOCH LOSS 0201] loss: 2.7729\n",
      "[EPOCH LOSS 0301] loss: 2.8243\n",
      "[EPOCH LOSS 0401] loss: 2.4824\n",
      "[EPOCH LOSS 0501] loss: 2.6350\n",
      "[EPOCH LOSS 0601] loss: 2.7592\n",
      "[EPOCH LOSS 0701] loss: 2.6385\n",
      "[EPOCH LOSS 0801] loss: 2.6289\n",
      "[EPOCH LOSS 0901] loss: 3.0178\n",
      "[EPOCH LOSS 1001] loss: 2.6068\n",
      "[EPOCH LOSS 1101] loss: 2.6099\n",
      "[EPOCH LOSS 1201] loss: 2.7412\n",
      "[EPOCH LOSS 1301] loss: 2.6447\n",
      "[EPOCH LOSS 1401] loss: 2.5071\n",
      "[EPOCH LOSS 1501] loss: 2.5039\n",
      "[EPOCH LOSS 1601] loss: 2.5402\n",
      "[EPOCH LOSS 1701] loss: 2.8540\n",
      "[EPOCH LOSS 1801] loss: 2.4633\n",
      "[EPOCH LOSS 1901] loss: 3.0963\n",
      "[EPOCH LOSS 2001] loss: 2.5887\n",
      "[EPOCH LOSS 2101] loss: 2.5670\n",
      "[EPOCH LOSS 2201] loss: 2.5627\n",
      "[EPOCH LOSS 2301] loss: 2.6470\n",
      "[EPOCH LOSS 2401] loss: 2.6647\n",
      "[EPOCH LOSS 2501] loss: 2.4209\n",
      "[EPOCH LOSS 2601] loss: 2.6114\n",
      "[EPOCH LOSS 2701] loss: 2.4511\n",
      "[EPOCH LOSS 2801] loss: 2.5648\n",
      "[EPOCH LOSS 2901] loss: 2.4593\n",
      "[EPOCH LOSS 3001] loss: 2.6376\n",
      "[EPOCH LOSS 3101] loss: 2.4395\n",
      "[EPOCH LOSS 3201] loss: 2.6121\n",
      "[EPOCH LOSS 3301] loss: 2.5026\n",
      "[EPOCH LOSS 3401] loss: 2.4715\n",
      "[EPOCH LOSS 3501] loss: 2.6920\n",
      "[EPOCH LOSS 3601] loss: 2.8724\n",
      "[EPOCH LOSS 3701] loss: 2.6325\n",
      "[EPOCH LOSS 3801] loss: 3.0103\n",
      "[EPOCH LOSS 3901] loss: 2.5876\n",
      "[EPOCH LOSS 4001] loss: 2.4948\n",
      "[EPOCH LOSS 4101] loss: 2.7879\n",
      "[EPOCH LOSS 4201] loss: 2.7485\n",
      "[EPOCH LOSS 4301] loss: 3.6387\n",
      "[EPOCH LOSS 4401] loss: 3.0503\n",
      "[EPOCH LOSS 4501] loss: 2.4608\n",
      "[EPOCH LOSS 4601] loss: 2.7972\n",
      "[EPOCH LOSS 4701] loss: 2.7853\n",
      "[EPOCH LOSS 4801] loss: 2.6288\n",
      "[EPOCH LOSS 4901] loss: 2.4152\n",
      "[EPOCH LOSS 5001] loss: 2.5366\n",
      "[EPOCH LOSS 5101] loss: 2.5798\n",
      "[EPOCH LOSS 5201] loss: 2.4070\n",
      "[EPOCH LOSS 5301] loss: 2.6958\n",
      "[EPOCH LOSS 5401] loss: 2.6577\n",
      "[EPOCH LOSS 5501] loss: 2.7359\n",
      "[EPOCH LOSS 5601] loss: 2.5033\n",
      "[EPOCH LOSS 5701] loss: 2.4614\n",
      "[EPOCH LOSS 5801] loss: 2.7341\n",
      "[EPOCH LOSS 5901] loss: 2.5109\n",
      "[EPOCH LOSS 6001] loss: 2.3952\n",
      "[EPOCH LOSS 6101] loss: 2.5444\n",
      "[EPOCH LOSS 6201] loss: 2.4494\n",
      "[EPOCH LOSS 6301] loss: 2.7409\n",
      "[EPOCH LOSS 6401] loss: 2.9667\n",
      "[EPOCH LOSS 6501] loss: 2.3809\n",
      "[EPOCH LOSS 6601] loss: 3.1946\n",
      "[EPOCH LOSS 6701] loss: 2.4715\n",
      "[EPOCH LOSS 6801] loss: 2.3623\n",
      "[EPOCH LOSS 6901] loss: 2.4780\n",
      "[EPOCH LOSS 7001] loss: 2.4099\n",
      "[EPOCH LOSS 7101] loss: 3.3119\n",
      "[EPOCH LOSS 7201] loss: 2.5884\n",
      "[EPOCH LOSS 7301] loss: 2.6869\n",
      "[EPOCH LOSS 7401] loss: 2.6225\n",
      "[EPOCH LOSS 7501] loss: 2.3979\n",
      "[EPOCH LOSS 7601] loss: 2.7255\n",
      "[EPOCH LOSS 7701] loss: 2.5589\n",
      "[EPOCH LOSS 7801] loss: 2.4232\n",
      "[EPOCH LOSS 7901] loss: 2.3586\n",
      "[EPOCH LOSS 8001] loss: 2.5573\n",
      "[EPOCH LOSS 8101] loss: 2.3979\n",
      "[EPOCH LOSS 8201] loss: 2.4941\n",
      "[EPOCH LOSS 8301] loss: 2.6030\n",
      "[EPOCH LOSS 8401] loss: 2.5451\n",
      "[EPOCH LOSS 8501] loss: 2.5555\n",
      "[EPOCH LOSS 8601] loss: 2.5324\n",
      "[EPOCH LOSS 8701] loss: 2.2930\n",
      "[EPOCH LOSS 8801] loss: 2.5143\n",
      "[EPOCH LOSS 8901] loss: 2.6752\n",
      "[EPOCH LOSS 9001] loss: 2.4163\n",
      "[EPOCH LOSS 9101] loss: 2.8945\n",
      "[EPOCH LOSS 9201] loss: 2.3801\n",
      "[EPOCH LOSS 9301] loss: 2.4454\n",
      "[EPOCH LOSS 9401] loss: 2.4246\n",
      "[EPOCH LOSS 9501] loss: 2.4681\n",
      "[EPOCH LOSS 9601] loss: 2.7492\n",
      "[EPOCH LOSS 9701] loss: 2.4556\n",
      "[EPOCH LOSS 9801] loss: 2.9622\n",
      "[EPOCH LOSS 9901] loss: 2.3232\n",
      "[EPOCH LOSS 10001] loss: 2.3612\n",
      "[EPOCH LOSS 10101] loss: 2.4053\n",
      "[EPOCH LOSS 10201] loss: 2.3757\n",
      "[EPOCH LOSS 10301] loss: 2.4064\n",
      "[EPOCH LOSS 10401] loss: 2.3812\n",
      "[EPOCH LOSS 10501] loss: 2.5537\n",
      "[EPOCH LOSS 10601] loss: 2.5112\n",
      "[EPOCH LOSS 10701] loss: 2.4062\n",
      "[EPOCH LOSS 10801] loss: 2.5337\n",
      "[EPOCH LOSS 10901] loss: 2.4551\n",
      "[EPOCH LOSS 11001] loss: 2.5065\n",
      "[EPOCH LOSS 11101] loss: 2.5488\n",
      "[EPOCH LOSS 11201] loss: 2.7535\n",
      "[EPOCH LOSS 11301] loss: 2.3384\n",
      "[EPOCH LOSS 11401] loss: 2.3642\n",
      "[EPOCH LOSS 11501] loss: 2.3584\n",
      "[EPOCH LOSS 11601] loss: 2.3633\n",
      "[EPOCH LOSS 11701] loss: 2.9246\n",
      "[EPOCH LOSS 11801] loss: 2.9532\n",
      "[EPOCH LOSS 11901] loss: 2.5368\n",
      "[EPOCH LOSS 12001] loss: 2.4484\n",
      "[EPOCH LOSS 12101] loss: 2.3959\n",
      "[EPOCH LOSS 12201] loss: 2.2033\n",
      "[EPOCH LOSS 12301] loss: 2.7599\n",
      "[EPOCH LOSS 12401] loss: 2.4275\n",
      "[EPOCH LOSS 12501] loss: 2.3847\n",
      "[EPOCH LOSS 12601] loss: 2.5035\n",
      "[EPOCH LOSS 12701] loss: 2.3596\n",
      "[EPOCH LOSS 12801] loss: 2.4131\n",
      "[EPOCH LOSS 12901] loss: 2.5296\n",
      "[EPOCH LOSS 13001] loss: 2.6250\n",
      "[EPOCH LOSS 13101] loss: 2.3982\n",
      "[EPOCH LOSS 13201] loss: 2.3342\n",
      "[EPOCH LOSS 13301] loss: 2.6027\n",
      "[EPOCH LOSS 13401] loss: 2.8048\n",
      "[EPOCH LOSS 13501] loss: 2.6184\n",
      "[EPOCH LOSS 13601] loss: 2.3125\n",
      "[EPOCH LOSS 13701] loss: 2.7529\n",
      "[EPOCH LOSS 13801] loss: 2.2788\n",
      "[EPOCH LOSS 13901] loss: 2.2978\n",
      "[EPOCH LOSS 14001] loss: 2.2906\n",
      "[EPOCH LOSS 14101] loss: 2.5181\n",
      "[EPOCH LOSS 14201] loss: 2.5221\n",
      "[EPOCH LOSS 14301] loss: 2.4836\n",
      "[EPOCH LOSS 14401] loss: 2.2701\n",
      "[EPOCH LOSS 14501] loss: 2.4152\n",
      "[EPOCH LOSS 14601] loss: 2.4088\n",
      "[EPOCH LOSS 14701] loss: 2.5205\n",
      "[EPOCH LOSS 14801] loss: 2.4662\n",
      "[EPOCH LOSS 14901] loss: 2.3291\n",
      "[EPOCH LOSS 15001] loss: 2.3569\n",
      "[EPOCH LOSS 15101] loss: 2.2905\n",
      "[EPOCH LOSS 15201] loss: 2.2998\n",
      "[EPOCH LOSS 15301] loss: 2.3513\n",
      "[EPOCH LOSS 15401] loss: 2.4800\n",
      "[EPOCH LOSS 15501] loss: 2.2724\n",
      "[EPOCH LOSS 15601] loss: 2.9250\n",
      "[EPOCH LOSS 15701] loss: 2.6302\n",
      "[EPOCH LOSS 15801] loss: 2.6514\n",
      "[EPOCH LOSS 15901] loss: 2.5507\n",
      "[EPOCH LOSS 16001] loss: 2.7638\n",
      "[EPOCH LOSS 16101] loss: 2.5915\n",
      "[EPOCH LOSS 16201] loss: 2.3129\n",
      "[EPOCH LOSS 16301] loss: 2.3298\n",
      "[EPOCH LOSS 16401] loss: 2.3657\n",
      "[EPOCH LOSS 16501] loss: 2.5414\n",
      "[EPOCH LOSS 16601] loss: 2.2887\n",
      "[EPOCH LOSS 16701] loss: 2.4756\n",
      "[EPOCH LOSS 16801] loss: 2.5611\n",
      "[EPOCH LOSS 16901] loss: 2.5743\n",
      "[EPOCH LOSS 17001] loss: 2.5118\n",
      "[EPOCH LOSS 17101] loss: 2.3756\n",
      "[EPOCH LOSS 17201] loss: 2.8175\n",
      "[EPOCH LOSS 17301] loss: 2.3037\n",
      "[EPOCH LOSS 17401] loss: 2.4213\n",
      "[EPOCH LOSS 17501] loss: 2.3405\n",
      "[EPOCH LOSS 17601] loss: 2.3196\n",
      "[EPOCH LOSS 17701] loss: 2.3170\n",
      "[EPOCH LOSS 17801] loss: 2.3606\n",
      "[EPOCH LOSS 17901] loss: 2.3641\n",
      "[EPOCH LOSS 18001] loss: 2.3148\n",
      "[EPOCH LOSS 18101] loss: 2.3562\n",
      "[EPOCH LOSS 18201] loss: 2.3281\n",
      "[EPOCH LOSS 18301] loss: 2.5056\n",
      "[EPOCH LOSS 18401] loss: 2.4616\n",
      "[EPOCH LOSS 18501] loss: 2.3930\n",
      "[EPOCH LOSS 18601] loss: 2.4537\n",
      "[EPOCH LOSS 18701] loss: 2.3244\n",
      "[EPOCH LOSS 18801] loss: 2.4144\n",
      "[EPOCH LOSS 18901] loss: 3.0278\n",
      "[EPOCH LOSS 19001] loss: 2.6714\n",
      "[EPOCH LOSS 19101] loss: 2.4715\n",
      "[EPOCH LOSS 19201] loss: 2.3049\n",
      "[EPOCH LOSS 19301] loss: 2.4238\n",
      "[EPOCH LOSS 19401] loss: 2.5937\n",
      "[EPOCH LOSS 19501] loss: 2.4294\n",
      "[EPOCH LOSS 19601] loss: 2.3103\n",
      "[EPOCH LOSS 19701] loss: 2.2769\n",
      "[EPOCH LOSS 19801] loss: 2.6248\n",
      "[EPOCH LOSS 19901] loss: 2.4577\n",
      "[EPOCH LOSS 20001] loss: 2.7882\n",
      "[EPOCH LOSS 20101] loss: 2.3912\n",
      "[EPOCH LOSS 20201] loss: 2.2127\n",
      "[EPOCH LOSS 20301] loss: 2.3800\n",
      "[EPOCH LOSS 20401] loss: 2.3443\n",
      "[EPOCH LOSS 20501] loss: 2.2293\n",
      "[EPOCH LOSS 20601] loss: 2.4730\n",
      "[EPOCH LOSS 20701] loss: 2.4474\n",
      "[EPOCH LOSS 20801] loss: 2.3549\n",
      "[EPOCH LOSS 20901] loss: 2.7436\n",
      "[EPOCH LOSS 21001] loss: 2.4359\n",
      "[EPOCH LOSS 21101] loss: 2.5084\n",
      "[EPOCH LOSS 21201] loss: 2.3281\n",
      "[EPOCH LOSS 21301] loss: 2.3263\n",
      "[EPOCH LOSS 21401] loss: 2.5154\n",
      "[EPOCH LOSS 21501] loss: 2.4448\n",
      "[EPOCH LOSS 21601] loss: 2.2777\n",
      "[EPOCH LOSS 21701] loss: 2.3104\n",
      "[EPOCH LOSS 21801] loss: 2.3306\n",
      "[EPOCH LOSS 21901] loss: 2.8528\n",
      "[EPOCH LOSS 22001] loss: 2.5296\n",
      "[EPOCH LOSS 22101] loss: 2.4318\n",
      "[EPOCH LOSS 22201] loss: 2.3908\n",
      "[EPOCH LOSS 22301] loss: 2.7729\n",
      "[EPOCH LOSS 22401] loss: 2.2653\n",
      "[EPOCH LOSS 22501] loss: 2.2910\n",
      "[EPOCH LOSS 22601] loss: 2.4766\n",
      "[EPOCH LOSS 22701] loss: 2.5531\n",
      "[EPOCH LOSS 22801] loss: 2.2795\n",
      "[EPOCH LOSS 22901] loss: 2.6637\n",
      "[EPOCH LOSS 23001] loss: 2.3165\n",
      "[EPOCH LOSS 23101] loss: 2.3365\n",
      "[EPOCH LOSS 23201] loss: 2.3218\n",
      "[EPOCH LOSS 23301] loss: 2.3733\n",
      "[EPOCH LOSS 23401] loss: 2.2851\n",
      "[EPOCH LOSS 23501] loss: 2.5078\n",
      "[EPOCH LOSS 23601] loss: 2.3302\n",
      "[EPOCH LOSS 23701] loss: 2.4121\n",
      "[EPOCH LOSS 23801] loss: 2.3761\n",
      "[EPOCH LOSS 23901] loss: 2.3075\n",
      "[EPOCH LOSS 24001] loss: 2.6257\n",
      "[EPOCH LOSS 24101] loss: 2.6451\n",
      "[EPOCH LOSS 24201] loss: 2.4479\n",
      "[EPOCH LOSS 24301] loss: 2.3565\n",
      "[EPOCH LOSS 24401] loss: 3.5696\n",
      "[EPOCH LOSS 24501] loss: 2.5991\n",
      "[EPOCH LOSS 24601] loss: 2.7839\n",
      "[EPOCH LOSS 24701] loss: 2.2745\n",
      "[EPOCH LOSS 24801] loss: 2.3129\n",
      "[EPOCH LOSS 24901] loss: 2.5695\n",
      "[EPOCH LOSS 25001] loss: 2.7524\n",
      "[EPOCH LOSS 25101] loss: 2.3393\n",
      "[EPOCH LOSS 25201] loss: 2.2999\n",
      "[EPOCH LOSS 25301] loss: 2.5354\n",
      "[EPOCH LOSS 25401] loss: 2.3404\n",
      "[EPOCH LOSS 25501] loss: 2.5053\n",
      "[EPOCH LOSS 25601] loss: 2.2878\n",
      "[EPOCH LOSS 25701] loss: 2.3813\n",
      "[EPOCH LOSS 25801] loss: 2.4076\n",
      "[EPOCH LOSS 25901] loss: 2.4544\n",
      "[EPOCH LOSS 26001] loss: 2.2487\n",
      "[EPOCH LOSS 26101] loss: 2.2624\n",
      "[EPOCH LOSS 26201] loss: 2.3517\n",
      "[EPOCH LOSS 26301] loss: 2.3902\n",
      "[EPOCH LOSS 26401] loss: 2.3537\n",
      "[EPOCH LOSS 26501] loss: 2.3840\n",
      "[EPOCH LOSS 26601] loss: 2.2527\n",
      "[EPOCH LOSS 26701] loss: 2.5100\n",
      "[EPOCH LOSS 26801] loss: 2.4089\n",
      "[EPOCH LOSS 26901] loss: 2.4021\n",
      "[EPOCH LOSS 27001] loss: 2.3159\n",
      "[EPOCH LOSS 27101] loss: 2.3718\n",
      "[EPOCH LOSS 27201] loss: 2.3317\n",
      "[EPOCH LOSS 27301] loss: 2.5881\n",
      "[EPOCH LOSS 27401] loss: 2.3915\n",
      "[EPOCH LOSS 27501] loss: 2.3263\n",
      "[EPOCH LOSS 27601] loss: 2.4142\n",
      "[EPOCH LOSS 27701] loss: 2.4700\n",
      "[EPOCH LOSS 27801] loss: 2.5357\n",
      "[EPOCH LOSS 27901] loss: 2.4300\n",
      "[EPOCH LOSS 28001] loss: 2.3572\n",
      "[EPOCH LOSS 28101] loss: 2.3726\n",
      "[EPOCH LOSS 28201] loss: 2.4354\n",
      "[EPOCH LOSS 28301] loss: 2.6794\n",
      "[EPOCH LOSS 28401] loss: 2.4156\n",
      "[EPOCH LOSS 28501] loss: 2.4885\n",
      "[EPOCH LOSS 28601] loss: 2.3853\n",
      "[EPOCH LOSS 28701] loss: 2.4528\n",
      "[EPOCH LOSS 28801] loss: 2.9495\n",
      "[EPOCH LOSS 28901] loss: 2.2086\n",
      "[EPOCH LOSS 29001] loss: 2.3847\n",
      "[EPOCH LOSS 29101] loss: 2.5908\n",
      "[EPOCH LOSS 29201] loss: 2.5170\n",
      "[EPOCH LOSS 29301] loss: 2.6383\n",
      "[EPOCH LOSS 29401] loss: 2.5370\n",
      "[EPOCH LOSS 29501] loss: 2.4309\n",
      "[EPOCH LOSS 29601] loss: 2.8762\n",
      "[EPOCH LOSS 29701] loss: 2.2877\n",
      "[EPOCH LOSS 29801] loss: 2.3411\n",
      "[EPOCH LOSS 29901] loss: 2.2356\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "num_steps = 30000\n",
    "initial_lr = 0.001\n",
    "gamma = 0.1  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "optim = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "svi = SVI(random_model.model, random_guide, optim, loss=Trace_ELBO())\n",
    "\n",
    "for j in range(num_steps):\n",
    "    loss = 0\n",
    "    for data_batch in R_grouped_train_loader:\n",
    "        grp_descrp, grp_energy, logic_reduce = data_batch[0][10], data_batch[0][11], data_batch[0][12]\n",
    "        grp_descrp[0] = grp_descrp[0].float()\n",
    "        grp_descrp[1] = grp_descrp[1].float()\n",
    "\n",
    "        logic_reduce[0] = logic_reduce[0].float()\n",
    "        logic_reduce[1] = logic_reduce[1].float()\n",
    "        loss += svi.step(grp_descrp, logic_reduce, grp_energy) \n",
    "    if j % 100 == 0:\n",
    "        print(\"[EPOCH LOSS %04d] loss: %.4f\" % (j + 1, loss / len(logic_reduce[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del R_grouped_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD Validation Set before random choice: 2.6130075268125155\n",
      "STD Validation Set after choice: 1.1125461442639568\n"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(random_model.model, guide=random_guide, num_samples=10000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "\n",
    "R_v_samples = predictive(v_grp_descrp, v_logic_reduce)\n",
    "R_v_std = torch.std(R_v_samples['_RETURN'], 0).numpy()\n",
    "\n",
    "\n",
    "print('STD Validation Set before random choice: {}'.format(np.mean(v_std)))\n",
    "print('STD Validation Set after choice: {}'.format(np.mean(R_v_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.6072, 2.8733, 2.3040, 3.5480, 4.0700], dtype=torch.float64)\n",
      "tensor([3.5480, 2.1885, 2.5797, 2.4523, 2.5608], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(bnn.model, guide=guide, num_samples=10000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "t_samples = predictive(t_grp_descrp, t_logic_reduce)\n",
    "print(torch.std(t_samples['obs'][:,sorted_std_test[-5:]], dim=0))\n",
    "print(torch.std(t_samples['obs'][:,shuffle_test[-5:]], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8253, 1.6166, 1.5502, 2.1510, 2.9217], dtype=torch.float64)\n",
      "tensor([2.1510, 1.6516, 1.5571, 1.6355, 1.5723], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(random_model.model, guide=random_guide, num_samples=10000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "t_samples = predictive(t_grp_descrp, t_logic_reduce)\n",
    "print(torch.std(t_samples['obs'][:,sorted_std_test[-5:]], dim=0))\n",
    "print(torch.std(t_samples['obs'][:,shuffle_test[-5:]], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8165, 1.6238, 1.5754, 2.1741, 2.9024], dtype=torch.float64)\n",
      "tensor([2.1741, 1.6356, 1.5649, 1.6779, 1.5773], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "predictive = Predictive(guided_model.model, guide=guided_guide, num_samples=10000,\n",
    "                        return_sites=(\"obs\", \"_RETURN\"))\n",
    "t_samples = predictive(t_grp_descrp, t_logic_reduce)\n",
    "print(torch.std(t_samples['obs'][:,sorted_std_test[-5:]], dim=0))\n",
    "print(torch.std(t_samples['obs'][:,shuffle_test[-5:]], dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
